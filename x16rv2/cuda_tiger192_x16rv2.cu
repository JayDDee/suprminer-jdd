/*
 * tiger-192 djm34
 * tiger-192 merged kernels (x16Rv2) sp september 2019. tiger+luffa512, tiger+sha512 , 
 * tiger+keccak and variants. Optimized the sharedmem access to reduce bank conflicts.
 */

/*
 * tiger-192 kernel implementation.
 *
 * ==========================(LICENSE BEGIN)============================
 *
 * Copyright (c) 2014  djm34
 * Copyright (c) 2019  sp
 *
 * Permission is hereby granted, free of charge, to any person obtaining
 * a copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be
 * included in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *
 * ===========================(LICENSE END)=============================
 *
 * @author   phm <phm@inbox.com>
 * @author   sp <https://github.com/sp-hash>
 */
#include <cuda.h>
#include "cuda_runtime.h"
#include "device_launch_parameters.h"
#include "cuda_helper_alexis.h"
#include "miner.h"
#include "cuda_vectors_alexis.h"


#include <stdio.h>
#include <stdint.h>
#include <memory.h>


#define SPH_C64(x)    (x ## ULL)
#define SPH_C32(x)    (x ## U)
#define SPH_T64(x)  (x)


__constant__ const uint64_t III[3] = {
	SPH_C64(0x0123456789ABCDEF),SPH_C64(0xFEDCBA9876543210),SPH_C64(0xF096A5B4C3B2E187)
};


__constant__ const uint64_t T1[256] = {
	SPH_C64(0x02AAB17CF7E90C5E), SPH_C64(0xAC424B03E243A8EC),
	SPH_C64(0x72CD5BE30DD5FCD3), SPH_C64(0x6D019B93F6F97F3A),
	SPH_C64(0xCD9978FFD21F9193), SPH_C64(0x7573A1C9708029E2),
	SPH_C64(0xB164326B922A83C3), SPH_C64(0x46883EEE04915870),
	SPH_C64(0xEAACE3057103ECE6), SPH_C64(0xC54169B808A3535C),
	SPH_C64(0x4CE754918DDEC47C), SPH_C64(0x0AA2F4DFDC0DF40C),
	SPH_C64(0x10B76F18A74DBEFA), SPH_C64(0xC6CCB6235AD1AB6A),
	SPH_C64(0x13726121572FE2FF), SPH_C64(0x1A488C6F199D921E),
	SPH_C64(0x4BC9F9F4DA0007CA), SPH_C64(0x26F5E6F6E85241C7),
	SPH_C64(0x859079DBEA5947B6), SPH_C64(0x4F1885C5C99E8C92),
	SPH_C64(0xD78E761EA96F864B), SPH_C64(0x8E36428C52B5C17D),
	SPH_C64(0x69CF6827373063C1), SPH_C64(0xB607C93D9BB4C56E),
	SPH_C64(0x7D820E760E76B5EA), SPH_C64(0x645C9CC6F07FDC42),
	SPH_C64(0xBF38A078243342E0), SPH_C64(0x5F6B343C9D2E7D04),
	SPH_C64(0xF2C28AEB600B0EC6), SPH_C64(0x6C0ED85F7254BCAC),
	SPH_C64(0x71592281A4DB4FE5), SPH_C64(0x1967FA69CE0FED9F),
	SPH_C64(0xFD5293F8B96545DB), SPH_C64(0xC879E9D7F2A7600B),
	SPH_C64(0x860248920193194E), SPH_C64(0xA4F9533B2D9CC0B3),
	SPH_C64(0x9053836C15957613), SPH_C64(0xDB6DCF8AFC357BF1),
	SPH_C64(0x18BEEA7A7A370F57), SPH_C64(0x037117CA50B99066),
	SPH_C64(0x6AB30A9774424A35), SPH_C64(0xF4E92F02E325249B),
	SPH_C64(0x7739DB07061CCAE1), SPH_C64(0xD8F3B49CECA42A05),
	SPH_C64(0xBD56BE3F51382F73), SPH_C64(0x45FAED5843B0BB28),
	SPH_C64(0x1C813D5C11BF1F83), SPH_C64(0x8AF0E4B6D75FA169),
	SPH_C64(0x33EE18A487AD9999), SPH_C64(0x3C26E8EAB1C94410),
	SPH_C64(0xB510102BC0A822F9), SPH_C64(0x141EEF310CE6123B),
	SPH_C64(0xFC65B90059DDB154), SPH_C64(0xE0158640C5E0E607),
	SPH_C64(0x884E079826C3A3CF), SPH_C64(0x930D0D9523C535FD),
	SPH_C64(0x35638D754E9A2B00), SPH_C64(0x4085FCCF40469DD5),
	SPH_C64(0xC4B17AD28BE23A4C), SPH_C64(0xCAB2F0FC6A3E6A2E),
	SPH_C64(0x2860971A6B943FCD), SPH_C64(0x3DDE6EE212E30446),
	SPH_C64(0x6222F32AE01765AE), SPH_C64(0x5D550BB5478308FE),
	SPH_C64(0xA9EFA98DA0EDA22A), SPH_C64(0xC351A71686C40DA7),
	SPH_C64(0x1105586D9C867C84), SPH_C64(0xDCFFEE85FDA22853),
	SPH_C64(0xCCFBD0262C5EEF76), SPH_C64(0xBAF294CB8990D201),
	SPH_C64(0xE69464F52AFAD975), SPH_C64(0x94B013AFDF133E14),
	SPH_C64(0x06A7D1A32823C958), SPH_C64(0x6F95FE5130F61119),
	SPH_C64(0xD92AB34E462C06C0), SPH_C64(0xED7BDE33887C71D2),
	SPH_C64(0x79746D6E6518393E), SPH_C64(0x5BA419385D713329),
	SPH_C64(0x7C1BA6B948A97564), SPH_C64(0x31987C197BFDAC67),
	SPH_C64(0xDE6C23C44B053D02), SPH_C64(0x581C49FED002D64D),
	SPH_C64(0xDD474D6338261571), SPH_C64(0xAA4546C3E473D062),
	SPH_C64(0x928FCE349455F860), SPH_C64(0x48161BBACAAB94D9),
	SPH_C64(0x63912430770E6F68), SPH_C64(0x6EC8A5E602C6641C),
	SPH_C64(0x87282515337DDD2B), SPH_C64(0x2CDA6B42034B701B),
	SPH_C64(0xB03D37C181CB096D), SPH_C64(0xE108438266C71C6F),
	SPH_C64(0x2B3180C7EB51B255), SPH_C64(0xDF92B82F96C08BBC),
	SPH_C64(0x5C68C8C0A632F3BA), SPH_C64(0x5504CC861C3D0556),
	SPH_C64(0xABBFA4E55FB26B8F), SPH_C64(0x41848B0AB3BACEB4),
	SPH_C64(0xB334A273AA445D32), SPH_C64(0xBCA696F0A85AD881),
	SPH_C64(0x24F6EC65B528D56C), SPH_C64(0x0CE1512E90F4524A),
	SPH_C64(0x4E9DD79D5506D35A), SPH_C64(0x258905FAC6CE9779),
	SPH_C64(0x2019295B3E109B33), SPH_C64(0xF8A9478B73A054CC),
	SPH_C64(0x2924F2F934417EB0), SPH_C64(0x3993357D536D1BC4),
	SPH_C64(0x38A81AC21DB6FF8B), SPH_C64(0x47C4FBF17D6016BF),
	SPH_C64(0x1E0FAADD7667E3F5), SPH_C64(0x7ABCFF62938BEB96),
	SPH_C64(0xA78DAD948FC179C9), SPH_C64(0x8F1F98B72911E50D),
	SPH_C64(0x61E48EAE27121A91), SPH_C64(0x4D62F7AD31859808),
	SPH_C64(0xECEBA345EF5CEAEB), SPH_C64(0xF5CEB25EBC9684CE),
	SPH_C64(0xF633E20CB7F76221), SPH_C64(0xA32CDF06AB8293E4),
	SPH_C64(0x985A202CA5EE2CA4), SPH_C64(0xCF0B8447CC8A8FB1),
	SPH_C64(0x9F765244979859A3), SPH_C64(0xA8D516B1A1240017),
	SPH_C64(0x0BD7BA3EBB5DC726), SPH_C64(0xE54BCA55B86ADB39),
	SPH_C64(0x1D7A3AFD6C478063), SPH_C64(0x519EC608E7669EDD),
	SPH_C64(0x0E5715A2D149AA23), SPH_C64(0x177D4571848FF194),
	SPH_C64(0xEEB55F3241014C22), SPH_C64(0x0F5E5CA13A6E2EC2),
	SPH_C64(0x8029927B75F5C361), SPH_C64(0xAD139FABC3D6E436),
	SPH_C64(0x0D5DF1A94CCF402F), SPH_C64(0x3E8BD948BEA5DFC8),
	SPH_C64(0xA5A0D357BD3FF77E), SPH_C64(0xA2D12E251F74F645),
	SPH_C64(0x66FD9E525E81A082), SPH_C64(0x2E0C90CE7F687A49),
	SPH_C64(0xC2E8BCBEBA973BC5), SPH_C64(0x000001BCE509745F),
	SPH_C64(0x423777BBE6DAB3D6), SPH_C64(0xD1661C7EAEF06EB5),
	SPH_C64(0xA1781F354DAACFD8), SPH_C64(0x2D11284A2B16AFFC),
	SPH_C64(0xF1FC4F67FA891D1F), SPH_C64(0x73ECC25DCB920ADA),
	SPH_C64(0xAE610C22C2A12651), SPH_C64(0x96E0A810D356B78A),
	SPH_C64(0x5A9A381F2FE7870F), SPH_C64(0xD5AD62EDE94E5530),
	SPH_C64(0xD225E5E8368D1427), SPH_C64(0x65977B70C7AF4631),
	SPH_C64(0x99F889B2DE39D74F), SPH_C64(0x233F30BF54E1D143),
	SPH_C64(0x9A9675D3D9A63C97), SPH_C64(0x5470554FF334F9A8),
	SPH_C64(0x166ACB744A4F5688), SPH_C64(0x70C74CAAB2E4AEAD),
	SPH_C64(0xF0D091646F294D12), SPH_C64(0x57B82A89684031D1),
	SPH_C64(0xEFD95A5A61BE0B6B), SPH_C64(0x2FBD12E969F2F29A),
	SPH_C64(0x9BD37013FEFF9FE8), SPH_C64(0x3F9B0404D6085A06),
	SPH_C64(0x4940C1F3166CFE15), SPH_C64(0x09542C4DCDF3DEFB),
	SPH_C64(0xB4C5218385CD5CE3), SPH_C64(0xC935B7DC4462A641),
	SPH_C64(0x3417F8A68ED3B63F), SPH_C64(0xB80959295B215B40),
	SPH_C64(0xF99CDAEF3B8C8572), SPH_C64(0x018C0614F8FCB95D),
	SPH_C64(0x1B14ACCD1A3ACDF3), SPH_C64(0x84D471F200BB732D),
	SPH_C64(0xC1A3110E95E8DA16), SPH_C64(0x430A7220BF1A82B8),
	SPH_C64(0xB77E090D39DF210E), SPH_C64(0x5EF4BD9F3CD05E9D),
	SPH_C64(0x9D4FF6DA7E57A444), SPH_C64(0xDA1D60E183D4A5F8),
	SPH_C64(0xB287C38417998E47), SPH_C64(0xFE3EDC121BB31886),
	SPH_C64(0xC7FE3CCC980CCBEF), SPH_C64(0xE46FB590189BFD03),
	SPH_C64(0x3732FD469A4C57DC), SPH_C64(0x7EF700A07CF1AD65),
	SPH_C64(0x59C64468A31D8859), SPH_C64(0x762FB0B4D45B61F6),
	SPH_C64(0x155BAED099047718), SPH_C64(0x68755E4C3D50BAA6),
	SPH_C64(0xE9214E7F22D8B4DF), SPH_C64(0x2ADDBF532EAC95F4),
	SPH_C64(0x32AE3909B4BD0109), SPH_C64(0x834DF537B08E3450),
	SPH_C64(0xFA209DA84220728D), SPH_C64(0x9E691D9B9EFE23F7),
	SPH_C64(0x0446D288C4AE8D7F), SPH_C64(0x7B4CC524E169785B),
	SPH_C64(0x21D87F0135CA1385), SPH_C64(0xCEBB400F137B8AA5),
	SPH_C64(0x272E2B66580796BE), SPH_C64(0x3612264125C2B0DE),
	SPH_C64(0x057702BDAD1EFBB2), SPH_C64(0xD4BABB8EACF84BE9),
	SPH_C64(0x91583139641BC67B), SPH_C64(0x8BDC2DE08036E024),
	SPH_C64(0x603C8156F49F68ED), SPH_C64(0xF7D236F7DBEF5111),
	SPH_C64(0x9727C4598AD21E80), SPH_C64(0xA08A0896670A5FD7),
	SPH_C64(0xCB4A8F4309EBA9CB), SPH_C64(0x81AF564B0F7036A1),
	SPH_C64(0xC0B99AA778199ABD), SPH_C64(0x959F1EC83FC8E952),
	SPH_C64(0x8C505077794A81B9), SPH_C64(0x3ACAAF8F056338F0),
	SPH_C64(0x07B43F50627A6778), SPH_C64(0x4A44AB49F5ECCC77),
	SPH_C64(0x3BC3D6E4B679EE98), SPH_C64(0x9CC0D4D1CF14108C),
	SPH_C64(0x4406C00B206BC8A0), SPH_C64(0x82A18854C8D72D89),
	SPH_C64(0x67E366B35C3C432C), SPH_C64(0xB923DD61102B37F2),
	SPH_C64(0x56AB2779D884271D), SPH_C64(0xBE83E1B0FF1525AF),
	SPH_C64(0xFB7C65D4217E49A9), SPH_C64(0x6BDBE0E76D48E7D4),
	SPH_C64(0x08DF828745D9179E), SPH_C64(0x22EA6A9ADD53BD34),
	SPH_C64(0xE36E141C5622200A), SPH_C64(0x7F805D1B8CB750EE),
	SPH_C64(0xAFE5C7A59F58E837), SPH_C64(0xE27F996A4FB1C23C),
	SPH_C64(0xD3867DFB0775F0D0), SPH_C64(0xD0E673DE6E88891A),
	SPH_C64(0x123AEB9EAFB86C25), SPH_C64(0x30F1D5D5C145B895),
	SPH_C64(0xBB434A2DEE7269E7), SPH_C64(0x78CB67ECF931FA38),
	SPH_C64(0xF33B0372323BBF9C), SPH_C64(0x52D66336FB279C74),
	SPH_C64(0x505F33AC0AFB4EAA), SPH_C64(0xE8A5CD99A2CCE187),
	SPH_C64(0x534974801E2D30BB), SPH_C64(0x8D2D5711D5876D90),
	SPH_C64(0x1F1A412891BC038E), SPH_C64(0xD6E2E71D82E56648),
	SPH_C64(0x74036C3A497732B7), SPH_C64(0x89B67ED96361F5AB),
	SPH_C64(0xFFED95D8F1EA02A2), SPH_C64(0xE72B3BD61464D43D),
	SPH_C64(0xA6300F170BDC4820), SPH_C64(0xEBC18760ED78A77A)
};

__constant__ const uint64_t T2[256] = {
	SPH_C64(0xE6A6BE5A05A12138), SPH_C64(0xB5A122A5B4F87C98),
	SPH_C64(0x563C6089140B6990), SPH_C64(0x4C46CB2E391F5DD5),
	SPH_C64(0xD932ADDBC9B79434), SPH_C64(0x08EA70E42015AFF5),
	SPH_C64(0xD765A6673E478CF1), SPH_C64(0xC4FB757EAB278D99),
	SPH_C64(0xDF11C6862D6E0692), SPH_C64(0xDDEB84F10D7F3B16),
	SPH_C64(0x6F2EF604A665EA04), SPH_C64(0x4A8E0F0FF0E0DFB3),
	SPH_C64(0xA5EDEEF83DBCBA51), SPH_C64(0xFC4F0A2A0EA4371E),
	SPH_C64(0xE83E1DA85CB38429), SPH_C64(0xDC8FF882BA1B1CE2),
	SPH_C64(0xCD45505E8353E80D), SPH_C64(0x18D19A00D4DB0717),
	SPH_C64(0x34A0CFEDA5F38101), SPH_C64(0x0BE77E518887CAF2),
	SPH_C64(0x1E341438B3C45136), SPH_C64(0xE05797F49089CCF9),
	SPH_C64(0xFFD23F9DF2591D14), SPH_C64(0x543DDA228595C5CD),
	SPH_C64(0x661F81FD99052A33), SPH_C64(0x8736E641DB0F7B76),
	SPH_C64(0x15227725418E5307), SPH_C64(0xE25F7F46162EB2FA),
	SPH_C64(0x48A8B2126C13D9FE), SPH_C64(0xAFDC541792E76EEA),
	SPH_C64(0x03D912BFC6D1898F), SPH_C64(0x31B1AAFA1B83F51B),
	SPH_C64(0xF1AC2796E42AB7D9), SPH_C64(0x40A3A7D7FCD2EBAC),
	SPH_C64(0x1056136D0AFBBCC5), SPH_C64(0x7889E1DD9A6D0C85),
	SPH_C64(0xD33525782A7974AA), SPH_C64(0xA7E25D09078AC09B),
	SPH_C64(0xBD4138B3EAC6EDD0), SPH_C64(0x920ABFBE71EB9E70),
	SPH_C64(0xA2A5D0F54FC2625C), SPH_C64(0xC054E36B0B1290A3),
	SPH_C64(0xF6DD59FF62FE932B), SPH_C64(0x3537354511A8AC7D),
	SPH_C64(0xCA845E9172FADCD4), SPH_C64(0x84F82B60329D20DC),
	SPH_C64(0x79C62CE1CD672F18), SPH_C64(0x8B09A2ADD124642C),
	SPH_C64(0xD0C1E96A19D9E726), SPH_C64(0x5A786A9B4BA9500C),
	SPH_C64(0x0E020336634C43F3), SPH_C64(0xC17B474AEB66D822),
	SPH_C64(0x6A731AE3EC9BAAC2), SPH_C64(0x8226667AE0840258),
	SPH_C64(0x67D4567691CAECA5), SPH_C64(0x1D94155C4875ADB5),
	SPH_C64(0x6D00FD985B813FDF), SPH_C64(0x51286EFCB774CD06),
	SPH_C64(0x5E8834471FA744AF), SPH_C64(0xF72CA0AEE761AE2E),
	SPH_C64(0xBE40E4CDAEE8E09A), SPH_C64(0xE9970BBB5118F665),
	SPH_C64(0x726E4BEB33DF1964), SPH_C64(0x703B000729199762),
	SPH_C64(0x4631D816F5EF30A7), SPH_C64(0xB880B5B51504A6BE),
	SPH_C64(0x641793C37ED84B6C), SPH_C64(0x7B21ED77F6E97D96),
	SPH_C64(0x776306312EF96B73), SPH_C64(0xAE528948E86FF3F4),
	SPH_C64(0x53DBD7F286A3F8F8), SPH_C64(0x16CADCE74CFC1063),
	SPH_C64(0x005C19BDFA52C6DD), SPH_C64(0x68868F5D64D46AD3),
	SPH_C64(0x3A9D512CCF1E186A), SPH_C64(0x367E62C2385660AE),
	SPH_C64(0xE359E7EA77DCB1D7), SPH_C64(0x526C0773749ABE6E),
	SPH_C64(0x735AE5F9D09F734B), SPH_C64(0x493FC7CC8A558BA8),
	SPH_C64(0xB0B9C1533041AB45), SPH_C64(0x321958BA470A59BD),
	SPH_C64(0x852DB00B5F46C393), SPH_C64(0x91209B2BD336B0E5),
	SPH_C64(0x6E604F7D659EF19F), SPH_C64(0xB99A8AE2782CCB24),
	SPH_C64(0xCCF52AB6C814C4C7), SPH_C64(0x4727D9AFBE11727B),
	SPH_C64(0x7E950D0C0121B34D), SPH_C64(0x756F435670AD471F),
	SPH_C64(0xF5ADD442615A6849), SPH_C64(0x4E87E09980B9957A),
	SPH_C64(0x2ACFA1DF50AEE355), SPH_C64(0xD898263AFD2FD556),
	SPH_C64(0xC8F4924DD80C8FD6), SPH_C64(0xCF99CA3D754A173A),
	SPH_C64(0xFE477BACAF91BF3C), SPH_C64(0xED5371F6D690C12D),
	SPH_C64(0x831A5C285E687094), SPH_C64(0xC5D3C90A3708A0A4),
	SPH_C64(0x0F7F903717D06580), SPH_C64(0x19F9BB13B8FDF27F),
	SPH_C64(0xB1BD6F1B4D502843), SPH_C64(0x1C761BA38FFF4012),
	SPH_C64(0x0D1530C4E2E21F3B), SPH_C64(0x8943CE69A7372C8A),
	SPH_C64(0xE5184E11FEB5CE66), SPH_C64(0x618BDB80BD736621),
	SPH_C64(0x7D29BAD68B574D0B), SPH_C64(0x81BB613E25E6FE5B),
	SPH_C64(0x071C9C10BC07913F), SPH_C64(0xC7BEEB7909AC2D97),
	SPH_C64(0xC3E58D353BC5D757), SPH_C64(0xEB017892F38F61E8),
	SPH_C64(0xD4EFFB9C9B1CC21A), SPH_C64(0x99727D26F494F7AB),
	SPH_C64(0xA3E063A2956B3E03), SPH_C64(0x9D4A8B9A4AA09C30),
	SPH_C64(0x3F6AB7D500090FB4), SPH_C64(0x9CC0F2A057268AC0),
	SPH_C64(0x3DEE9D2DEDBF42D1), SPH_C64(0x330F49C87960A972),
	SPH_C64(0xC6B2720287421B41), SPH_C64(0x0AC59EC07C00369C),
	SPH_C64(0xEF4EAC49CB353425), SPH_C64(0xF450244EEF0129D8),
	SPH_C64(0x8ACC46E5CAF4DEB6), SPH_C64(0x2FFEAB63989263F7),
	SPH_C64(0x8F7CB9FE5D7A4578), SPH_C64(0x5BD8F7644E634635),
	SPH_C64(0x427A7315BF2DC900), SPH_C64(0x17D0C4AA2125261C),
	SPH_C64(0x3992486C93518E50), SPH_C64(0xB4CBFEE0A2D7D4C3),
	SPH_C64(0x7C75D6202C5DDD8D), SPH_C64(0xDBC295D8E35B6C61),
	SPH_C64(0x60B369D302032B19), SPH_C64(0xCE42685FDCE44132),
	SPH_C64(0x06F3DDB9DDF65610), SPH_C64(0x8EA4D21DB5E148F0),
	SPH_C64(0x20B0FCE62FCD496F), SPH_C64(0x2C1B912358B0EE31),
	SPH_C64(0xB28317B818F5A308), SPH_C64(0xA89C1E189CA6D2CF),
	SPH_C64(0x0C6B18576AAADBC8), SPH_C64(0xB65DEAA91299FAE3),
	SPH_C64(0xFB2B794B7F1027E7), SPH_C64(0x04E4317F443B5BEB),
	SPH_C64(0x4B852D325939D0A6), SPH_C64(0xD5AE6BEEFB207FFC),
	SPH_C64(0x309682B281C7D374), SPH_C64(0xBAE309A194C3B475),
	SPH_C64(0x8CC3F97B13B49F05), SPH_C64(0x98A9422FF8293967),
	SPH_C64(0x244B16B01076FF7C), SPH_C64(0xF8BF571C663D67EE),
	SPH_C64(0x1F0D6758EEE30DA1), SPH_C64(0xC9B611D97ADEB9B7),
	SPH_C64(0xB7AFD5887B6C57A2), SPH_C64(0x6290AE846B984FE1),
	SPH_C64(0x94DF4CDEACC1A5FD), SPH_C64(0x058A5BD1C5483AFF),
	SPH_C64(0x63166CC142BA3C37), SPH_C64(0x8DB8526EB2F76F40),
	SPH_C64(0xE10880036F0D6D4E), SPH_C64(0x9E0523C9971D311D),
	SPH_C64(0x45EC2824CC7CD691), SPH_C64(0x575B8359E62382C9),
	SPH_C64(0xFA9E400DC4889995), SPH_C64(0xD1823ECB45721568),
	SPH_C64(0xDAFD983B8206082F), SPH_C64(0xAA7D29082386A8CB),
	SPH_C64(0x269FCD4403B87588), SPH_C64(0x1B91F5F728BDD1E0),
	SPH_C64(0xE4669F39040201F6), SPH_C64(0x7A1D7C218CF04ADE),
	SPH_C64(0x65623C29D79CE5CE), SPH_C64(0x2368449096C00BB1),
	SPH_C64(0xAB9BF1879DA503BA), SPH_C64(0xBC23ECB1A458058E),
	SPH_C64(0x9A58DF01BB401ECC), SPH_C64(0xA070E868A85F143D),
	SPH_C64(0x4FF188307DF2239E), SPH_C64(0x14D565B41A641183),
	SPH_C64(0xEE13337452701602), SPH_C64(0x950E3DCF3F285E09),
	SPH_C64(0x59930254B9C80953), SPH_C64(0x3BF299408930DA6D),
	SPH_C64(0xA955943F53691387), SPH_C64(0xA15EDECAA9CB8784),
	SPH_C64(0x29142127352BE9A0), SPH_C64(0x76F0371FFF4E7AFB),
	SPH_C64(0x0239F450274F2228), SPH_C64(0xBB073AF01D5E868B),
	SPH_C64(0xBFC80571C10E96C1), SPH_C64(0xD267088568222E23),
	SPH_C64(0x9671A3D48E80B5B0), SPH_C64(0x55B5D38AE193BB81),
	SPH_C64(0x693AE2D0A18B04B8), SPH_C64(0x5C48B4ECADD5335F),
	SPH_C64(0xFD743B194916A1CA), SPH_C64(0x2577018134BE98C4),
	SPH_C64(0xE77987E83C54A4AD), SPH_C64(0x28E11014DA33E1B9),
	SPH_C64(0x270CC59E226AA213), SPH_C64(0x71495F756D1A5F60),
	SPH_C64(0x9BE853FB60AFEF77), SPH_C64(0xADC786A7F7443DBF),
	SPH_C64(0x0904456173B29A82), SPH_C64(0x58BC7A66C232BD5E),
	SPH_C64(0xF306558C673AC8B2), SPH_C64(0x41F639C6B6C9772A),
	SPH_C64(0x216DEFE99FDA35DA), SPH_C64(0x11640CC71C7BE615),
	SPH_C64(0x93C43694565C5527), SPH_C64(0xEA038E6246777839),
	SPH_C64(0xF9ABF3CE5A3E2469), SPH_C64(0x741E768D0FD312D2),
	SPH_C64(0x0144B883CED652C6), SPH_C64(0xC20B5A5BA33F8552),
	SPH_C64(0x1AE69633C3435A9D), SPH_C64(0x97A28CA4088CFDEC),
	SPH_C64(0x8824A43C1E96F420), SPH_C64(0x37612FA66EEEA746),
	SPH_C64(0x6B4CB165F9CF0E5A), SPH_C64(0x43AA1C06A0ABFB4A),
	SPH_C64(0x7F4DC26FF162796B), SPH_C64(0x6CBACC8E54ED9B0F),
	SPH_C64(0xA6B7FFEFD2BB253E), SPH_C64(0x2E25BC95B0A29D4F),
	SPH_C64(0x86D6A58BDEF1388C), SPH_C64(0xDED74AC576B6F054),
	SPH_C64(0x8030BDBC2B45805D), SPH_C64(0x3C81AF70E94D9289),
	SPH_C64(0x3EFF6DDA9E3100DB), SPH_C64(0xB38DC39FDFCC8847),
	SPH_C64(0x123885528D17B87E), SPH_C64(0xF2DA0ED240B1B642),
	SPH_C64(0x44CEFADCD54BF9A9), SPH_C64(0x1312200E433C7EE6),
	SPH_C64(0x9FFCC84F3A78C748), SPH_C64(0xF0CD1F72248576BB),
	SPH_C64(0xEC6974053638CFE4), SPH_C64(0x2BA7B67C0CEC4E4C),
	SPH_C64(0xAC2F4DF3E5CE32ED), SPH_C64(0xCB33D14326EA4C11),
	SPH_C64(0xA4E9044CC77E58BC), SPH_C64(0x5F513293D934FCEF),
	SPH_C64(0x5DC9645506E55444), SPH_C64(0x50DE418F317DE40A),
	SPH_C64(0x388CB31A69DDE259), SPH_C64(0x2DB4A83455820A86),
	SPH_C64(0x9010A91E84711AE9), SPH_C64(0x4DF7F0B7B1498371),
	SPH_C64(0xD62A2EABC0977179), SPH_C64(0x22FAC097AA8D5C0E)
};

__constant__ const uint64_t T3[256] = {
	SPH_C64(0xF49FCC2FF1DAF39B), SPH_C64(0x487FD5C66FF29281),
	SPH_C64(0xE8A30667FCDCA83F), SPH_C64(0x2C9B4BE3D2FCCE63),
	SPH_C64(0xDA3FF74B93FBBBC2), SPH_C64(0x2FA165D2FE70BA66),
	SPH_C64(0xA103E279970E93D4), SPH_C64(0xBECDEC77B0E45E71),
	SPH_C64(0xCFB41E723985E497), SPH_C64(0xB70AAA025EF75017),
	SPH_C64(0xD42309F03840B8E0), SPH_C64(0x8EFC1AD035898579),
	SPH_C64(0x96C6920BE2B2ABC5), SPH_C64(0x66AF4163375A9172),
	SPH_C64(0x2174ABDCCA7127FB), SPH_C64(0xB33CCEA64A72FF41),
	SPH_C64(0xF04A4933083066A5), SPH_C64(0x8D970ACDD7289AF5),
	SPH_C64(0x8F96E8E031C8C25E), SPH_C64(0xF3FEC02276875D47),
	SPH_C64(0xEC7BF310056190DD), SPH_C64(0xF5ADB0AEBB0F1491),
	SPH_C64(0x9B50F8850FD58892), SPH_C64(0x4975488358B74DE8),
	SPH_C64(0xA3354FF691531C61), SPH_C64(0x0702BBE481D2C6EE),
	SPH_C64(0x89FB24057DEDED98), SPH_C64(0xAC3075138596E902),
	SPH_C64(0x1D2D3580172772ED), SPH_C64(0xEB738FC28E6BC30D),
	SPH_C64(0x5854EF8F63044326), SPH_C64(0x9E5C52325ADD3BBE),
	SPH_C64(0x90AA53CF325C4623), SPH_C64(0xC1D24D51349DD067),
	SPH_C64(0x2051CFEEA69EA624), SPH_C64(0x13220F0A862E7E4F),
	SPH_C64(0xCE39399404E04864), SPH_C64(0xD9C42CA47086FCB7),
	SPH_C64(0x685AD2238A03E7CC), SPH_C64(0x066484B2AB2FF1DB),
	SPH_C64(0xFE9D5D70EFBF79EC), SPH_C64(0x5B13B9DD9C481854),
	SPH_C64(0x15F0D475ED1509AD), SPH_C64(0x0BEBCD060EC79851),
	SPH_C64(0xD58C6791183AB7F8), SPH_C64(0xD1187C5052F3EEE4),
	SPH_C64(0xC95D1192E54E82FF), SPH_C64(0x86EEA14CB9AC6CA2),
	SPH_C64(0x3485BEB153677D5D), SPH_C64(0xDD191D781F8C492A),
	SPH_C64(0xF60866BAA784EBF9), SPH_C64(0x518F643BA2D08C74),
	SPH_C64(0x8852E956E1087C22), SPH_C64(0xA768CB8DC410AE8D),
	SPH_C64(0x38047726BFEC8E1A), SPH_C64(0xA67738B4CD3B45AA),
	SPH_C64(0xAD16691CEC0DDE19), SPH_C64(0xC6D4319380462E07),
	SPH_C64(0xC5A5876D0BA61938), SPH_C64(0x16B9FA1FA58FD840),
	SPH_C64(0x188AB1173CA74F18), SPH_C64(0xABDA2F98C99C021F),
	SPH_C64(0x3E0580AB134AE816), SPH_C64(0x5F3B05B773645ABB),
	SPH_C64(0x2501A2BE5575F2F6), SPH_C64(0x1B2F74004E7E8BA9),
	SPH_C64(0x1CD7580371E8D953), SPH_C64(0x7F6ED89562764E30),
	SPH_C64(0xB15926FF596F003D), SPH_C64(0x9F65293DA8C5D6B9),
	SPH_C64(0x6ECEF04DD690F84C), SPH_C64(0x4782275FFF33AF88),
	SPH_C64(0xE41433083F820801), SPH_C64(0xFD0DFE409A1AF9B5),
	SPH_C64(0x4325A3342CDB396B), SPH_C64(0x8AE77E62B301B252),
	SPH_C64(0xC36F9E9F6655615A), SPH_C64(0x85455A2D92D32C09),
	SPH_C64(0xF2C7DEA949477485), SPH_C64(0x63CFB4C133A39EBA),
	SPH_C64(0x83B040CC6EBC5462), SPH_C64(0x3B9454C8FDB326B0),
	SPH_C64(0x56F56A9E87FFD78C), SPH_C64(0x2DC2940D99F42BC6),
	SPH_C64(0x98F7DF096B096E2D), SPH_C64(0x19A6E01E3AD852BF),
	SPH_C64(0x42A99CCBDBD4B40B), SPH_C64(0xA59998AF45E9C559),
	SPH_C64(0x366295E807D93186), SPH_C64(0x6B48181BFAA1F773),
	SPH_C64(0x1FEC57E2157A0A1D), SPH_C64(0x4667446AF6201AD5),
	SPH_C64(0xE615EBCACFB0F075), SPH_C64(0xB8F31F4F68290778),
	SPH_C64(0x22713ED6CE22D11E), SPH_C64(0x3057C1A72EC3C93B),
	SPH_C64(0xCB46ACC37C3F1F2F), SPH_C64(0xDBB893FD02AAF50E),
	SPH_C64(0x331FD92E600B9FCF), SPH_C64(0xA498F96148EA3AD6),
	SPH_C64(0xA8D8426E8B6A83EA), SPH_C64(0xA089B274B7735CDC),
	SPH_C64(0x87F6B3731E524A11), SPH_C64(0x118808E5CBC96749),
	SPH_C64(0x9906E4C7B19BD394), SPH_C64(0xAFED7F7E9B24A20C),
	SPH_C64(0x6509EADEEB3644A7), SPH_C64(0x6C1EF1D3E8EF0EDE),
	SPH_C64(0xB9C97D43E9798FB4), SPH_C64(0xA2F2D784740C28A3),
	SPH_C64(0x7B8496476197566F), SPH_C64(0x7A5BE3E6B65F069D),
	SPH_C64(0xF96330ED78BE6F10), SPH_C64(0xEEE60DE77A076A15),
	SPH_C64(0x2B4BEE4AA08B9BD0), SPH_C64(0x6A56A63EC7B8894E),
	SPH_C64(0x02121359BA34FEF4), SPH_C64(0x4CBF99F8283703FC),
	SPH_C64(0x398071350CAF30C8), SPH_C64(0xD0A77A89F017687A),
	SPH_C64(0xF1C1A9EB9E423569), SPH_C64(0x8C7976282DEE8199),
	SPH_C64(0x5D1737A5DD1F7ABD), SPH_C64(0x4F53433C09A9FA80),
	SPH_C64(0xFA8B0C53DF7CA1D9), SPH_C64(0x3FD9DCBC886CCB77),
	SPH_C64(0xC040917CA91B4720), SPH_C64(0x7DD00142F9D1DCDF),
	SPH_C64(0x8476FC1D4F387B58), SPH_C64(0x23F8E7C5F3316503),
	SPH_C64(0x032A2244E7E37339), SPH_C64(0x5C87A5D750F5A74B),
	SPH_C64(0x082B4CC43698992E), SPH_C64(0xDF917BECB858F63C),
	SPH_C64(0x3270B8FC5BF86DDA), SPH_C64(0x10AE72BB29B5DD76),
	SPH_C64(0x576AC94E7700362B), SPH_C64(0x1AD112DAC61EFB8F),
	SPH_C64(0x691BC30EC5FAA427), SPH_C64(0xFF246311CC327143),
	SPH_C64(0x3142368E30E53206), SPH_C64(0x71380E31E02CA396),
	SPH_C64(0x958D5C960AAD76F1), SPH_C64(0xF8D6F430C16DA536),
	SPH_C64(0xC8FFD13F1BE7E1D2), SPH_C64(0x7578AE66004DDBE1),
	SPH_C64(0x05833F01067BE646), SPH_C64(0xBB34B5AD3BFE586D),
	SPH_C64(0x095F34C9A12B97F0), SPH_C64(0x247AB64525D60CA8),
	SPH_C64(0xDCDBC6F3017477D1), SPH_C64(0x4A2E14D4DECAD24D),
	SPH_C64(0xBDB5E6D9BE0A1EEB), SPH_C64(0x2A7E70F7794301AB),
	SPH_C64(0xDEF42D8A270540FD), SPH_C64(0x01078EC0A34C22C1),
	SPH_C64(0xE5DE511AF4C16387), SPH_C64(0x7EBB3A52BD9A330A),
	SPH_C64(0x77697857AA7D6435), SPH_C64(0x004E831603AE4C32),
	SPH_C64(0xE7A21020AD78E312), SPH_C64(0x9D41A70C6AB420F2),
	SPH_C64(0x28E06C18EA1141E6), SPH_C64(0xD2B28CBD984F6B28),
	SPH_C64(0x26B75F6C446E9D83), SPH_C64(0xBA47568C4D418D7F),
	SPH_C64(0xD80BADBFE6183D8E), SPH_C64(0x0E206D7F5F166044),
	SPH_C64(0xE258A43911CBCA3E), SPH_C64(0x723A1746B21DC0BC),
	SPH_C64(0xC7CAA854F5D7CDD3), SPH_C64(0x7CAC32883D261D9C),
	SPH_C64(0x7690C26423BA942C), SPH_C64(0x17E55524478042B8),
	SPH_C64(0xE0BE477656A2389F), SPH_C64(0x4D289B5E67AB2DA0),
	SPH_C64(0x44862B9C8FBBFD31), SPH_C64(0xB47CC8049D141365),
	SPH_C64(0x822C1B362B91C793), SPH_C64(0x4EB14655FB13DFD8),
	SPH_C64(0x1ECBBA0714E2A97B), SPH_C64(0x6143459D5CDE5F14),
	SPH_C64(0x53A8FBF1D5F0AC89), SPH_C64(0x97EA04D81C5E5B00),
	SPH_C64(0x622181A8D4FDB3F3), SPH_C64(0xE9BCD341572A1208),
	SPH_C64(0x1411258643CCE58A), SPH_C64(0x9144C5FEA4C6E0A4),
	SPH_C64(0x0D33D06565CF620F), SPH_C64(0x54A48D489F219CA1),
	SPH_C64(0xC43E5EAC6D63C821), SPH_C64(0xA9728B3A72770DAF),
	SPH_C64(0xD7934E7B20DF87EF), SPH_C64(0xE35503B61A3E86E5),
	SPH_C64(0xCAE321FBC819D504), SPH_C64(0x129A50B3AC60BFA6),
	SPH_C64(0xCD5E68EA7E9FB6C3), SPH_C64(0xB01C90199483B1C7),
	SPH_C64(0x3DE93CD5C295376C), SPH_C64(0xAED52EDF2AB9AD13),
	SPH_C64(0x2E60F512C0A07884), SPH_C64(0xBC3D86A3E36210C9),
	SPH_C64(0x35269D9B163951CE), SPH_C64(0x0C7D6E2AD0CDB5FA),
	SPH_C64(0x59E86297D87F5733), SPH_C64(0x298EF221898DB0E7),
	SPH_C64(0x55000029D1A5AA7E), SPH_C64(0x8BC08AE1B5061B45),
	SPH_C64(0xC2C31C2B6C92703A), SPH_C64(0x94CC596BAF25EF42),
	SPH_C64(0x0A1D73DB22540456), SPH_C64(0x04B6A0F9D9C4179A),
	SPH_C64(0xEFFDAFA2AE3D3C60), SPH_C64(0xF7C8075BB49496C4),
	SPH_C64(0x9CC5C7141D1CD4E3), SPH_C64(0x78BD1638218E5534),
	SPH_C64(0xB2F11568F850246A), SPH_C64(0xEDFABCFA9502BC29),
	SPH_C64(0x796CE5F2DA23051B), SPH_C64(0xAAE128B0DC93537C),
	SPH_C64(0x3A493DA0EE4B29AE), SPH_C64(0xB5DF6B2C416895D7),
	SPH_C64(0xFCABBD25122D7F37), SPH_C64(0x70810B58105DC4B1),
	SPH_C64(0xE10FDD37F7882A90), SPH_C64(0x524DCAB5518A3F5C),
	SPH_C64(0x3C9E85878451255B), SPH_C64(0x4029828119BD34E2),
	SPH_C64(0x74A05B6F5D3CECCB), SPH_C64(0xB610021542E13ECA),
	SPH_C64(0x0FF979D12F59E2AC), SPH_C64(0x6037DA27E4F9CC50),
	SPH_C64(0x5E92975A0DF1847D), SPH_C64(0xD66DE190D3E623FE),
	SPH_C64(0x5032D6B87B568048), SPH_C64(0x9A36B7CE8235216E),
	SPH_C64(0x80272A7A24F64B4A), SPH_C64(0x93EFED8B8C6916F7),
	SPH_C64(0x37DDBFF44CCE1555), SPH_C64(0x4B95DB5D4B99BD25),
	SPH_C64(0x92D3FDA169812FC0), SPH_C64(0xFB1A4A9A90660BB6),
	SPH_C64(0x730C196946A4B9B2), SPH_C64(0x81E289AA7F49DA68),
	SPH_C64(0x64669A0F83B1A05F), SPH_C64(0x27B3FF7D9644F48B),
	SPH_C64(0xCC6B615C8DB675B3), SPH_C64(0x674F20B9BCEBBE95),
	SPH_C64(0x6F31238275655982), SPH_C64(0x5AE488713E45CF05),
	SPH_C64(0xBF619F9954C21157), SPH_C64(0xEABAC46040A8EAE9),
	SPH_C64(0x454C6FE9F2C0C1CD), SPH_C64(0x419CF6496412691C),
	SPH_C64(0xD3DC3BEF265B0F70), SPH_C64(0x6D0E60F5C3578A9E)
};

__constant__ const uint64_t T4[256] = {
	SPH_C64(0x5B0E608526323C55), SPH_C64(0x1A46C1A9FA1B59F5),
	SPH_C64(0xA9E245A17C4C8FFA), SPH_C64(0x65CA5159DB2955D7),
	SPH_C64(0x05DB0A76CE35AFC2), SPH_C64(0x81EAC77EA9113D45),
	SPH_C64(0x528EF88AB6AC0A0D), SPH_C64(0xA09EA253597BE3FF),
	SPH_C64(0x430DDFB3AC48CD56), SPH_C64(0xC4B3A67AF45CE46F),
	SPH_C64(0x4ECECFD8FBE2D05E), SPH_C64(0x3EF56F10B39935F0),
	SPH_C64(0x0B22D6829CD619C6), SPH_C64(0x17FD460A74DF2069),
	SPH_C64(0x6CF8CC8E8510ED40), SPH_C64(0xD6C824BF3A6ECAA7),
	SPH_C64(0x61243D581A817049), SPH_C64(0x048BACB6BBC163A2),
	SPH_C64(0xD9A38AC27D44CC32), SPH_C64(0x7FDDFF5BAAF410AB),
	SPH_C64(0xAD6D495AA804824B), SPH_C64(0xE1A6A74F2D8C9F94),
	SPH_C64(0xD4F7851235DEE8E3), SPH_C64(0xFD4B7F886540D893),
	SPH_C64(0x247C20042AA4BFDA), SPH_C64(0x096EA1C517D1327C),
	SPH_C64(0xD56966B4361A6685), SPH_C64(0x277DA5C31221057D),
	SPH_C64(0x94D59893A43ACFF7), SPH_C64(0x64F0C51CCDC02281),
	SPH_C64(0x3D33BCC4FF6189DB), SPH_C64(0xE005CB184CE66AF1),
	SPH_C64(0xFF5CCD1D1DB99BEA), SPH_C64(0xB0B854A7FE42980F),
	SPH_C64(0x7BD46A6A718D4B9F), SPH_C64(0xD10FA8CC22A5FD8C),
	SPH_C64(0xD31484952BE4BD31), SPH_C64(0xC7FA975FCB243847),
	SPH_C64(0x4886ED1E5846C407), SPH_C64(0x28CDDB791EB70B04),
	SPH_C64(0xC2B00BE2F573417F), SPH_C64(0x5C9590452180F877),
	SPH_C64(0x7A6BDDFFF370EB00), SPH_C64(0xCE509E38D6D9D6A4),
	SPH_C64(0xEBEB0F00647FA702), SPH_C64(0x1DCC06CF76606F06),
	SPH_C64(0xE4D9F28BA286FF0A), SPH_C64(0xD85A305DC918C262),
	SPH_C64(0x475B1D8732225F54), SPH_C64(0x2D4FB51668CCB5FE),
	SPH_C64(0xA679B9D9D72BBA20), SPH_C64(0x53841C0D912D43A5),
	SPH_C64(0x3B7EAA48BF12A4E8), SPH_C64(0x781E0E47F22F1DDF),
	SPH_C64(0xEFF20CE60AB50973), SPH_C64(0x20D261D19DFFB742),
	SPH_C64(0x16A12B03062A2E39), SPH_C64(0x1960EB2239650495),
	SPH_C64(0x251C16FED50EB8B8), SPH_C64(0x9AC0C330F826016E),
	SPH_C64(0xED152665953E7671), SPH_C64(0x02D63194A6369570),
	SPH_C64(0x5074F08394B1C987), SPH_C64(0x70BA598C90B25CE1),
	SPH_C64(0x794A15810B9742F6), SPH_C64(0x0D5925E9FCAF8C6C),
	SPH_C64(0x3067716CD868744E), SPH_C64(0x910AB077E8D7731B),
	SPH_C64(0x6A61BBDB5AC42F61), SPH_C64(0x93513EFBF0851567),
	SPH_C64(0xF494724B9E83E9D5), SPH_C64(0xE887E1985C09648D),
	SPH_C64(0x34B1D3C675370CFD), SPH_C64(0xDC35E433BC0D255D),
	SPH_C64(0xD0AAB84234131BE0), SPH_C64(0x08042A50B48B7EAF),
	SPH_C64(0x9997C4EE44A3AB35), SPH_C64(0x829A7B49201799D0),
	SPH_C64(0x263B8307B7C54441), SPH_C64(0x752F95F4FD6A6CA6),
	SPH_C64(0x927217402C08C6E5), SPH_C64(0x2A8AB754A795D9EE),
	SPH_C64(0xA442F7552F72943D), SPH_C64(0x2C31334E19781208),
	SPH_C64(0x4FA98D7CEAEE6291), SPH_C64(0x55C3862F665DB309),
	SPH_C64(0xBD0610175D53B1F3), SPH_C64(0x46FE6CB840413F27),
	SPH_C64(0x3FE03792DF0CFA59), SPH_C64(0xCFE700372EB85E8F),
	SPH_C64(0xA7BE29E7ADBCE118), SPH_C64(0xE544EE5CDE8431DD),
	SPH_C64(0x8A781B1B41F1873E), SPH_C64(0xA5C94C78A0D2F0E7),
	SPH_C64(0x39412E2877B60728), SPH_C64(0xA1265EF3AFC9A62C),
	SPH_C64(0xBCC2770C6A2506C5), SPH_C64(0x3AB66DD5DCE1CE12),
	SPH_C64(0xE65499D04A675B37), SPH_C64(0x7D8F523481BFD216),
	SPH_C64(0x0F6F64FCEC15F389), SPH_C64(0x74EFBE618B5B13C8),
	SPH_C64(0xACDC82B714273E1D), SPH_C64(0xDD40BFE003199D17),
	SPH_C64(0x37E99257E7E061F8), SPH_C64(0xFA52626904775AAA),
	SPH_C64(0x8BBBF63A463D56F9), SPH_C64(0xF0013F1543A26E64),
	SPH_C64(0xA8307E9F879EC898), SPH_C64(0xCC4C27A4150177CC),
	SPH_C64(0x1B432F2CCA1D3348), SPH_C64(0xDE1D1F8F9F6FA013),
	SPH_C64(0x606602A047A7DDD6), SPH_C64(0xD237AB64CC1CB2C7),
	SPH_C64(0x9B938E7225FCD1D3), SPH_C64(0xEC4E03708E0FF476),
	SPH_C64(0xFEB2FBDA3D03C12D), SPH_C64(0xAE0BCED2EE43889A),
	SPH_C64(0x22CB8923EBFB4F43), SPH_C64(0x69360D013CF7396D),
	SPH_C64(0x855E3602D2D4E022), SPH_C64(0x073805BAD01F784C),
	SPH_C64(0x33E17A133852F546), SPH_C64(0xDF4874058AC7B638),
	SPH_C64(0xBA92B29C678AA14A), SPH_C64(0x0CE89FC76CFAADCD),
	SPH_C64(0x5F9D4E0908339E34), SPH_C64(0xF1AFE9291F5923B9),
	SPH_C64(0x6E3480F60F4A265F), SPH_C64(0xEEBF3A2AB29B841C),
	SPH_C64(0xE21938A88F91B4AD), SPH_C64(0x57DFEFF845C6D3C3),
	SPH_C64(0x2F006B0BF62CAAF2), SPH_C64(0x62F479EF6F75EE78),
	SPH_C64(0x11A55AD41C8916A9), SPH_C64(0xF229D29084FED453),
	SPH_C64(0x42F1C27B16B000E6), SPH_C64(0x2B1F76749823C074),
	SPH_C64(0x4B76ECA3C2745360), SPH_C64(0x8C98F463B91691BD),
	SPH_C64(0x14BCC93CF1ADE66A), SPH_C64(0x8885213E6D458397),
	SPH_C64(0x8E177DF0274D4711), SPH_C64(0xB49B73B5503F2951),
	SPH_C64(0x10168168C3F96B6B), SPH_C64(0x0E3D963B63CAB0AE),
	SPH_C64(0x8DFC4B5655A1DB14), SPH_C64(0xF789F1356E14DE5C),
	SPH_C64(0x683E68AF4E51DAC1), SPH_C64(0xC9A84F9D8D4B0FD9),
	SPH_C64(0x3691E03F52A0F9D1), SPH_C64(0x5ED86E46E1878E80),
	SPH_C64(0x3C711A0E99D07150), SPH_C64(0x5A0865B20C4E9310),
	SPH_C64(0x56FBFC1FE4F0682E), SPH_C64(0xEA8D5DE3105EDF9B),
	SPH_C64(0x71ABFDB12379187A), SPH_C64(0x2EB99DE1BEE77B9C),
	SPH_C64(0x21ECC0EA33CF4523), SPH_C64(0x59A4D7521805C7A1),
	SPH_C64(0x3896F5EB56AE7C72), SPH_C64(0xAA638F3DB18F75DC),
	SPH_C64(0x9F39358DABE9808E), SPH_C64(0xB7DEFA91C00B72AC),
	SPH_C64(0x6B5541FD62492D92), SPH_C64(0x6DC6DEE8F92E4D5B),
	SPH_C64(0x353F57ABC4BEEA7E), SPH_C64(0x735769D6DA5690CE),
	SPH_C64(0x0A234AA642391484), SPH_C64(0xF6F9508028F80D9D),
	SPH_C64(0xB8E319A27AB3F215), SPH_C64(0x31AD9C1151341A4D),
	SPH_C64(0x773C22A57BEF5805), SPH_C64(0x45C7561A07968633),
	SPH_C64(0xF913DA9E249DBE36), SPH_C64(0xDA652D9B78A64C68),
	SPH_C64(0x4C27A97F3BC334EF), SPH_C64(0x76621220E66B17F4),
	SPH_C64(0x967743899ACD7D0B), SPH_C64(0xF3EE5BCAE0ED6782),
	SPH_C64(0x409F753600C879FC), SPH_C64(0x06D09A39B5926DB6),
	SPH_C64(0x6F83AEB0317AC588), SPH_C64(0x01E6CA4A86381F21),
	SPH_C64(0x66FF3462D19F3025), SPH_C64(0x72207C24DDFD3BFB),
	SPH_C64(0x4AF6B6D3E2ECE2EB), SPH_C64(0x9C994DBEC7EA08DE),
	SPH_C64(0x49ACE597B09A8BC4), SPH_C64(0xB38C4766CF0797BA),
	SPH_C64(0x131B9373C57C2A75), SPH_C64(0xB1822CCE61931E58),
	SPH_C64(0x9D7555B909BA1C0C), SPH_C64(0x127FAFDD937D11D2),
	SPH_C64(0x29DA3BADC66D92E4), SPH_C64(0xA2C1D57154C2ECBC),
	SPH_C64(0x58C5134D82F6FE24), SPH_C64(0x1C3AE3515B62274F),
	SPH_C64(0xE907C82E01CB8126), SPH_C64(0xF8ED091913E37FCB),
	SPH_C64(0x3249D8F9C80046C9), SPH_C64(0x80CF9BEDE388FB63),
	SPH_C64(0x1881539A116CF19E), SPH_C64(0x5103F3F76BD52457),
	SPH_C64(0x15B7E6F5AE47F7A8), SPH_C64(0xDBD7C6DED47E9CCF),
	SPH_C64(0x44E55C410228BB1A), SPH_C64(0xB647D4255EDB4E99),
	SPH_C64(0x5D11882BB8AAFC30), SPH_C64(0xF5098BBB29D3212A),
	SPH_C64(0x8FB5EA14E90296B3), SPH_C64(0x677B942157DD025A),
	SPH_C64(0xFB58E7C0A390ACB5), SPH_C64(0x89D3674C83BD4A01),
	SPH_C64(0x9E2DA4DF4BF3B93B), SPH_C64(0xFCC41E328CAB4829),
	SPH_C64(0x03F38C96BA582C52), SPH_C64(0xCAD1BDBD7FD85DB2),
	SPH_C64(0xBBB442C16082AE83), SPH_C64(0xB95FE86BA5DA9AB0),
	SPH_C64(0xB22E04673771A93F), SPH_C64(0x845358C9493152D8),
	SPH_C64(0xBE2A488697B4541E), SPH_C64(0x95A2DC2DD38E6966),
	SPH_C64(0xC02C11AC923C852B), SPH_C64(0x2388B1990DF2A87B),
	SPH_C64(0x7C8008FA1B4F37BE), SPH_C64(0x1F70D0C84D54E503),
	SPH_C64(0x5490ADEC7ECE57D4), SPH_C64(0x002B3C27D9063A3A),
	SPH_C64(0x7EAEA3848030A2BF), SPH_C64(0xC602326DED2003C0),
	SPH_C64(0x83A7287D69A94086), SPH_C64(0xC57A5FCB30F57A8A),
	SPH_C64(0xB56844E479EBE779), SPH_C64(0xA373B40F05DCBCE9),
	SPH_C64(0xD71A786E88570EE2), SPH_C64(0x879CBACDBDE8F6A0),
	SPH_C64(0x976AD1BCC164A32F), SPH_C64(0xAB21E25E9666D78B),
	SPH_C64(0x901063AAE5E5C33C), SPH_C64(0x9818B34448698D90),
	SPH_C64(0xE36487AE3E1E8ABB), SPH_C64(0xAFBDF931893BDCB4),
	SPH_C64(0x6345A0DC5FBBD519), SPH_C64(0x8628FE269B9465CA),
	SPH_C64(0x1E5D01603F9C51EC), SPH_C64(0x4DE44006A15049B7),
	SPH_C64(0xBF6C70E5F776CBB1), SPH_C64(0x411218F2EF552BED),
	SPH_C64(0xCB0C0708705A36A3), SPH_C64(0xE74D14754F986044),
	SPH_C64(0xCD56D9430EA8280E), SPH_C64(0xC12591D7535F5065),
	SPH_C64(0xC83223F1720AEF96), SPH_C64(0xC3A0396F7363A51F)
};


#define BYTE(x, n) __byte_perm(((uint32_t*)&(x))[(n) / 4], 0, 0x4440 + ((n) % 4))

#define ROUND_RTX(a, b, c, x, mul)    { \
	uint64_t t0, t1; \
	c ^= x; \
	t0 = sharedMem[BYTE(c, 0)][index] ^ sharedMem[BYTE(c, 2)+256][index] ^ sharedMem[BYTE(c, 4)+512][index] ^ sharedMem[BYTE(c, 6)+768][index]; \
	t1 = sharedMem[BYTE(c, 7)][index] ^ sharedMem[BYTE(c, 5)+256][index] ^ sharedMem[BYTE(c, 3)+512][index] ^ sharedMem[BYTE(c, 1)+768][index]; \
	a -= t0; \
	b += t1; \
	b *= mul; \
}

#define ROUND(a, b, c, x, mul)    { \
	uint64_t t0, t1; \
	c ^= x; \
	t0 = sharedMem[BYTE(c, 0)] ^ sharedMem[BYTE(c, 2)+256] ^ sharedMem[BYTE(c, 4)+512] ^ __ldg(&T4[BYTE(c, 6)]); \
	t1 = sharedMem[BYTE(c, 7)] ^ sharedMem[BYTE(c, 5)+256] ^ sharedMem[BYTE(c, 3)+512] ^ __ldg(&T4[BYTE(c, 1)]); \
	a -= t0; \
	b += t1; \
	b *= mul; \
	}



#define PASS(a, b, c, mul)    { \
		ROUND(a, b, c, X0, mul); \
		ROUND(b, c, a, X1, mul); \
		ROUND(c, a, b, X2, mul); \
		ROUND(a, b, c, X3, mul); \
		ROUND(b, c, a, X4, mul); \
		ROUND(c, a, b, X5, mul); \
		ROUND(a, b, c, X6, mul); \
		ROUND(b, c, a, X7, mul); \
	}

#define PASS_RTX(a, b, c, mul)    { \
		ROUND_RTX(a, b, c, X0, mul); \
		ROUND_RTX(b, c, a, X1, mul); \
		ROUND_RTX(c, a, b, X2, mul); \
		ROUND_RTX(a, b, c, X3, mul); \
		ROUND_RTX(b, c, a, X4, mul); \
		ROUND_RTX(c, a, b, X5, mul); \
		ROUND_RTX(a, b, c, X6, mul); \
		ROUND_RTX(b, c, a, X7, mul); \
		}


#define KSCHED    { \
		X0 = SPH_T64(X0 - (X7 ^ SPH_C64(0xA5A5A5A5A5A5A5A5))); \
		X1 ^= X0; \
		X2 = SPH_T64(X2 + X1); \
		X3 = SPH_T64(X3 - (X2 ^ (~X1 << 19))); \
		X4 ^= X3; \
		X5 = SPH_T64(X5 + X4); \
		X6 = SPH_T64(X6 - (X5 ^ (~X4 >> 23))); \
		X7 ^= X6; \
		X0 = SPH_T64(X0 + X7); \
		X1 = SPH_T64(X1 - (X0 ^ (~X7 << 19))); \
		X2 ^= X1; \
		X3 = SPH_T64(X3 + X2); \
		X4 = SPH_T64(X4 - (X3 ^ (~X2 >> 23))); \
		X5 ^= X4; \
		X6 = SPH_T64(X6 + X5); \
		X7 = SPH_T64(X7 - (X6 ^ SPH_C64(0x0123456789ABCDEF))); \
	}


#define TIGER_ROUND_BODY(in, r)    { \
		uint64_t A, B, C; \
		uint64_t X0, X1, X2, X3, X4, X5, X6, X7; \
 \
		A = (r)[0]; \
		B = (r)[1]; \
		C = (r)[2]; \
 \
		X0 = (in[0]); \
		X1 = (in[1]); \
		X2 = (in[2]); \
		X3 = (in[3]); \
		X4 = (in[4]); \
		X5 = (in[5]); \
		X6 = (in[6]); \
		X7 = (in[7]); \
		PASS(A, B, C, 5); \
		KSCHED; \
		PASS(C, A, B, 7); \
		KSCHED; \
		PASS(B, C, A, 9); \
 \
		(r)[0] ^= A; \
		(r)[1] = SPH_T64(B - (r)[1]); \
		(r)[2] = SPH_T64(C + (r)[2]); \
	}

#define TIGER_ROUND_BODY_RTX(in, r)    { \
		uint64_t A, B, C; \
		uint64_t X0, X1, X2, X3, X4, X5, X6, X7; \
 \
		A = (r)[0]; \
		B = (r)[1]; \
		C = (r)[2]; \
 \
		X0 = (in[0]); \
		X1 = (in[1]); \
		X2 = (in[2]); \
		X3 = (in[3]); \
		X4 = (in[4]); \
		X5 = (in[5]); \
		X6 = (in[6]); \
		X7 = (in[7]); \
		PASS_RTX(A, B, C, 5); \
		KSCHED; \
		PASS_RTX(C, A, B, 7); \
		KSCHED; \
		PASS_RTX(B, C, A, 9); \
 \
		(r)[0] ^= A; \
		(r)[1] = SPH_T64(B - (r)[1]); \
		(r)[2] = SPH_T64(C + (r)[2]); \
		}

//Luffa512
static unsigned char PaddedMessage[128];
__constant__ uint64_t c_PaddedMessage80[10]; // padded message (80 bytes + padding)
__constant__ uint32_t _ALIGN(8) statebufferpre[8];
__constant__ uint32_t _ALIGN(8) statechainvpre[40];

#define MULT0(a) {\
	tmp = a[7]; \
	a[7] = a[6]; \
	a[6] = a[5]; \
	a[5] = a[4]; \
	a[4] = a[3] ^ tmp; \
	a[3] = a[2] ^ tmp; \
	a[2] = a[1]; \
	a[1] = a[0] ^ tmp; \
	a[0] = tmp; \
}

#define MULT2(a,j) {\
	tmp = a[7+(8*j)];\
	a[7+(8*j)] = a[6+(8*j)];\
	a[6+(8*j)] = a[5+(8*j)];\
	a[5+(8*j)] = a[4+(8*j)];\
	a[4+(8*j)] = a[3+(8*j)] ^ tmp;\
	a[3+(8*j)] = a[2+(8*j)] ^ tmp;\
	a[2+(8*j)] = a[1+(8*j)];\
	a[1+(8*j)] = a[0+(8*j)] ^ tmp;\
	a[0+(8*j)] = tmp;\
}

#define TWEAK(a0,a1,a2,a3,j)\
	a0 = ROTL32(a0,j);\
	a1 = ROTL32(a1,j);\
	a2 = ROTL32(a2,j);\
	a3 = ROTL32(a3,j);

#define STEP(c0,c1) {\
\
	uint32_t temp[ 2];\
	temp[ 0]  = chainv[0];\
	temp[ 1]  = chainv[ 5];\
	chainv[ 2] ^= chainv[ 3];\
	chainv[ 7] ^= chainv[ 4];\
	chainv[ 0] |= chainv[ 1];\
	chainv[ 5] |= chainv[ 6];\
	chainv[ 1]  = ~chainv[ 1];\
	chainv[ 6]  = ~chainv[ 6];\
	chainv[ 0] ^= chainv[ 3];\
	chainv[ 5] ^= chainv[ 4];\
	chainv[ 3] &= temp[ 0];\
	chainv[ 4] &= temp[ 1];\
	chainv[ 1] ^= chainv[ 3];\
	chainv[ 6] ^= chainv[ 4];\
	chainv[ 3] ^= chainv[ 2];\
	chainv[ 4] ^= chainv[ 7];\
	chainv[ 2] &= chainv[ 0];\
	chainv[ 7] &= chainv[ 5];\
	chainv[ 0]  = ~chainv[ 0];\
	chainv[ 5]  = ~chainv[ 5];\
	chainv[ 2] ^= chainv[ 1];\
	chainv[ 7] ^= chainv[ 6];\
	chainv[ 1] |= chainv[ 3];\
	chainv[ 6] |= chainv[ 4];\
	temp[ 0] ^= chainv[ 1];\
	temp[ 1] ^= chainv[ 6];\
	chainv[ 3] ^= chainv[ 2];\
	chainv[ 4] ^= chainv[ 7] ^ temp[ 0];\
	chainv[ 2] &= chainv[ 1];\
	chainv[ 7]  = (chainv[ 7] & chainv[ 6]) ^ chainv[ 3];\
	chainv[ 1] ^= chainv[ 0];\
	chainv[ 6] ^= chainv[ 5] ^ chainv[ 2];\
	chainv[ 5]  = chainv[ 1] ^ temp[ 1];\
	chainv[ 0]  = chainv[ 4] ^ ROTL32(temp[ 0],2); \
	chainv[ 1]  = chainv[ 5] ^ ROTL32(chainv[ 1],2); \
	chainv[ 2]  = chainv[ 6] ^ ROTL32(chainv[ 2],2); \
	chainv[ 3]  = chainv[ 7] ^ ROTL32(chainv[ 3],2); \
	chainv[ 4]  = chainv[ 0] ^ ROTL32(chainv[ 4],14); \
	chainv[ 5]  = chainv[ 1] ^ ROTL32(chainv[ 5],14); \
	chainv[ 6]  = chainv[ 2] ^ ROTL32(chainv[ 6],14); \
	chainv[ 7]  = chainv[ 3] ^ ROTL32(chainv[ 7],14); \
	chainv[ 0]  = chainv[ 4] ^ ROTL32(chainv[ 0],10) ^ c0; \
	chainv[ 1]  = chainv[ 5] ^ ROTL32(chainv[ 1],10); \
	chainv[ 2]  = chainv[ 6] ^ ROTL32(chainv[ 2],10); \
	chainv[ 3]  = chainv[ 7] ^ ROTL32(chainv[ 3],10); \
	chainv[ 4]  = ROTL32(chainv[ 4],1) ^ c1; \
	chainv[ 5]  = ROTL32(chainv[ 5],1); \
	chainv[ 6]  = ROTL32(chainv[ 6],1); \
	chainv[ 7]  = ROTL32(chainv[ 7],1); \
}

__device__ __forceinline__
void STEP2(uint32_t *t, const uint2 c0, const uint2 c1){
	uint32_t temp[4];
	temp[0] = t[0];
	temp[1] = t[5];
	temp[2] = t[0 + 8];
	temp[3] = t[8 + 5];
	t[2] ^= t[3];
	t[7] ^= t[4];
	t[8 + 2] ^= t[8 + 3];
	t[8 + 7] ^= t[8 + 4];
	t[0] |= t[1];
	t[5] |= t[6];
	t[8 + 0] |= t[8 + 1];
	t[8 + 5] |= t[8 + 6];
	t[1] = ~t[1];
	t[6] = ~t[6];
	t[8 + 1] = ~t[8 + 1];
	t[8 + 6] = ~t[8 + 6];
	t[0] ^= t[3];
	t[5] ^= t[4];
	t[8 + 0] ^= t[8 + 3];
	t[8 + 5] ^= t[8 + 4];
	t[3] &= temp[0];
	t[4] &= temp[1];
	t[8 + 3] &= temp[2];
	t[8 + 4] &= temp[3];
	t[1] ^= t[3];
	t[6] ^= t[4];
	t[8 + 1] ^= t[8 + 3];
	t[8 + 6] ^= t[8 + 4];
	t[3] ^= t[2];
	t[4] ^= t[7];
	t[8 + 3] ^= t[8 + 2];
	t[8 + 4] ^= t[8 + 7];
	t[2] &= t[0];
	t[7] &= t[5];
	t[8 + 2] &= t[8 + 0];
	t[8 + 7] &= t[8 + 5];
	t[0] = ~t[0];
	t[5] = ~t[5];
	t[8 + 0] = ~t[8 + 0];
	t[8 + 5] = ~t[8 + 5];
	t[2] ^= t[1];
	t[7] ^= t[6];
	t[8 + 2] ^= t[8 + 1];
	t[8 + 7] ^= t[8 + 6];
	t[1] |= t[3];
	t[6] |= t[4];
	t[8 + 1] |= t[8 + 3];
	t[8 + 6] |= t[8 + 4];

	temp[0] ^= t[1];
	temp[1] ^= t[6];
	temp[2] ^= t[8 + 1];
	temp[3] ^= t[8 + 6];

	t[3] ^= t[2];
	t[4] ^= t[7] ^ temp[0];
	t[8 + 3] ^= t[8 + 2];
	t[8 + 4] ^= t[8 + 7] ^ temp[2];
	t[2] &= t[1];
	t[7] = (t[7] & t[6]) ^ t[3];
	t[8 + 2] &= t[8 + 1];
	t[1] ^= t[0];
	t[8 + 7] = (t[8 + 6] & t[8 + 7]) ^ t[8 + 3];
	t[6] ^= t[5] ^ t[2];
	t[8 + 1] ^= t[8 + 0];
	t[8 + 6] ^= t[8 + 2] ^ t[8 + 5];
	t[5] = t[1] ^ temp[1];
	t[0] = t[4] ^ ROTL32(temp[0], 2);
	t[8 + 5] = t[8 + 1] ^ temp[3];
	t[8 + 0] = t[8 + 4] ^ ROTL32(temp[2], 2);
	t[1] = t[5] ^ ROTL32(t[1], 2);
	t[2] = t[6] ^ ROTL32(t[2], 2);
	t[8 + 1] = t[8 + 5] ^ ROTL32(t[8 + 1], 2);
	t[8 + 2] = t[8 + 6] ^ ROTL32(t[8 + 2], 2);
	t[3] = t[7] ^ ROTL32(t[3], 2);
	t[4] = t[0] ^ ROTL32(t[4], 14);
	t[8 + 3] = t[8 + 7] ^ ROTL32(t[8 + 3], 2);
	t[8 + 4] = t[8 + 0] ^ ROTL32(t[8 + 4], 14);
	t[5] = t[1] ^ ROTL32(t[5], 14);
	t[6] = t[2] ^ ROTL32(t[6], 14);
	t[8 + 5] = t[8 + 1] ^ ROTL32(t[8 + 5], 14);
	t[8 + 6] = t[8 + 2] ^ ROTL32(t[8 + 6], 14);
	t[7] = t[3] ^ ROTL32(t[7], 14);
	t[0] = t[4] ^ ROTL32(t[0], 10) ^ c0.x;
	t[8 + 7] = t[8 + 3] ^ ROTL32(t[8 + 7], 14);
	t[8 + 0] = t[8 + 4] ^ ROTL32(t[8 + 0], 10) ^ c1.x;
	t[1] = t[5] ^ ROTL32(t[1], 10);
	t[2] = t[6] ^ ROTL32(t[2], 10);
	t[8 + 1] = t[8 + 5] ^ ROTL32(t[8 + 1], 10);
	t[8 + 2] = t[8 + 6] ^ ROTL32(t[8 + 2], 10);
	t[3] = t[7] ^ ROTL32(t[3], 10);
	t[4] = ROTL32(t[4], 1) ^ c0.y;
	t[8 + 3] = t[8 + 7] ^ ROTL32(t[8 + 3], 10);
	t[8 + 4] = ROTL32(t[8 + 4], 1) ^ c1.y;
	t[5] = ROTL32(t[5], 1);
	t[6] = ROTL32(t[6], 1);
	t[8 + 5] = ROTL32(t[8 + 5], 1);
	t[8 + 6] = ROTL32(t[8 + 6], 1);
	t[7] = ROTL32(t[7], 1);
	t[8 + 7] = ROTL32(t[8 + 7], 1);
}

__device__ __forceinline__
void STEP1(uint32_t *t, const uint2 c){
	uint32_t temp[2];
	temp[0] = t[0];			temp[1] = t[5];
	t[2] ^= t[3];				t[7] ^= t[4];
	t[0] |= t[1];				t[5] |= t[6];
	t[1] = ~t[1];			t[6] = ~t[6];
	t[0] ^= t[3];				t[5] ^= t[4];
	t[3] &= temp[0];			t[4] &= temp[1];
	t[1] ^= t[3];				t[6] ^= t[4];
	t[3] ^= t[2];				t[4] ^= t[7];
	t[2] &= t[0];				t[7] &= t[5];
	t[0] = ~t[0];			t[5] = ~t[5];
	t[2] ^= t[1];				t[7] ^= t[6];
	t[1] |= t[3];				t[6] |= t[4];
	temp[0] ^= t[1];			temp[1] ^= t[6];
	t[3] ^= t[2];				t[4] ^= t[7] ^ temp[0];
	t[2] &= t[1];				t[7] = (t[7] & t[6]) ^ t[3];
	t[1] ^= t[0];				t[6] ^= t[5] ^ t[2];
	t[5] = t[1] ^ temp[1];		t[0] = t[4] ^ ROTL32(temp[0], 2);
	t[1] = t[5] ^ ROTL32(t[1], 2);	t[2] = t[6] ^ ROTL32(t[2], 2);
	t[3] = t[7] ^ ROTL32(t[3], 2);	t[4] = t[0] ^ ROTL32(t[4], 14);
	t[5] = t[1] ^ ROTL32(t[5], 14);	t[6] = t[2] ^ ROTL32(t[6], 14);
	t[7] = t[3] ^ ROTL32(t[7], 14);	t[0] = t[4] ^ ROTL32(t[0], 10) ^ c.x;
	t[1] = t[5] ^ ROTL32(t[1], 10);	t[2] = t[6] ^ ROTL32(t[2], 10);
	t[3] = t[7] ^ ROTL32(t[3], 10);	t[4] = ROTL32(t[4], 1) ^ c.y;
	t[5] = ROTL32(t[5], 1);		t[6] = ROTL32(t[6], 1);
	t[7] = ROTL32(t[7], 1);
}

/* initial values of chaining variables */
__constant__ const uint32_t c_CNS[80] = {
	0x303994a6, 0xe0337818, 0xc0e65299, 0x441ba90d, 0x6cc33a12, 0x7f34d442, 0xdc56983e, 0x9389217f, 0x1e00108f, 0xe5a8bce6, 0x7800423d, 0x5274baf4, 0x8f5b7882, 0x26889ba7, 0x96e1db12, 0x9a226e9d,
	0xb6de10ed, 0x01685f3d, 0x70f47aae, 0x05a17cf4, 0x0707a3d4, 0xbd09caca, 0x1c1e8f51, 0xf4272b28, 0x707a3d45, 0x144ae5cc, 0xaeb28562, 0xfaa7ae2b, 0xbaca1589, 0x2e48f1c1, 0x40a46f3e, 0xb923c704,
	0xfc20d9d2, 0xe25e72c1, 0x34552e25, 0xe623bb72, 0x7ad8818f, 0x5c58a4a4, 0x8438764a, 0x1e38e2e7, 0xbb6de032, 0x78e38b9d, 0xedb780c8, 0x27586719, 0xd9847356, 0x36eda57f, 0xa2c78434, 0x703aace7,
	0xb213afa5, 0xe028c9bf, 0xc84ebe95, 0x44756f91, 0x4e608a22, 0x7e8fce32, 0x56d858fe, 0x956548be, 0x343b138f, 0xfe191be2, 0xd0ec4e3d, 0x3cb226e5, 0x2ceb4882, 0x5944a28e, 0xb3ad2208, 0xa1c4c355,
	0xf0d2e9e3, 0x5090d577, 0xac11d7fa, 0x2d1925ab, 0x1bcb66f2, 0xb46496ac, 0x6f2d9bc9, 0xd1925ab0, 0x78602649, 0x29131ab6, 0x8edae952, 0x0fc053c3, 0x3b6ba548, 0x3f014f0c, 0xedae9520, 0xfc053c31
};

static uint32_t h_CNS[80] = {
	0x303994a6, 0xe0337818, 0xc0e65299, 0x441ba90d, 0x6cc33a12, 0x7f34d442, 0xdc56983e, 0x9389217f, 0x1e00108f, 0xe5a8bce6, 0x7800423d, 0x5274baf4, 0x8f5b7882, 0x26889ba7, 0x96e1db12, 0x9a226e9d,
	0xb6de10ed, 0x01685f3d, 0x70f47aae, 0x05a17cf4, 0x0707a3d4, 0xbd09caca, 0x1c1e8f51, 0xf4272b28, 0x707a3d45, 0x144ae5cc, 0xaeb28562, 0xfaa7ae2b, 0xbaca1589, 0x2e48f1c1, 0x40a46f3e, 0xb923c704,
	0xfc20d9d2, 0xe25e72c1, 0x34552e25, 0xe623bb72, 0x7ad8818f, 0x5c58a4a4, 0x8438764a, 0x1e38e2e7, 0xbb6de032, 0x78e38b9d, 0xedb780c8, 0x27586719, 0xd9847356, 0x36eda57f, 0xa2c78434, 0x703aace7,
	0xb213afa5, 0xe028c9bf, 0xc84ebe95, 0x44756f91, 0x4e608a22, 0x7e8fce32, 0x56d858fe, 0x956548be, 0x343b138f, 0xfe191be2, 0xd0ec4e3d, 0x3cb226e5, 0x2ceb4882, 0x5944a28e, 0xb3ad2208, 0xa1c4c355,
	0xf0d2e9e3, 0x5090d577, 0xac11d7fa, 0x2d1925ab, 0x1bcb66f2, 0xb46496ac, 0x6f2d9bc9, 0xd1925ab0, 0x78602649, 0x29131ab6, 0x8edae952, 0x0fc053c3, 0x3b6ba548, 0x3f014f0c, 0xedae9520, 0xfc053c31
};


__device__
static void rnd512(uint32_t *const __restrict__ statebuffer, uint32_t *const __restrict__ statechainv){
	uint32_t t[40];
	uint32_t tmp;

	tmp = statechainv[7] ^ statechainv[7 + 8] ^ statechainv[7 + 16] ^ statechainv[7 + 24] ^ statechainv[7 + 32];
	t[7] = statechainv[6] ^ statechainv[6 + 8] ^ statechainv[6 + 16] ^ statechainv[6 + 24] ^ statechainv[6 + 32];
	t[6] = statechainv[5] ^ statechainv[5 + 8] ^ statechainv[5 + 16] ^ statechainv[5 + 24] ^ statechainv[5 + 32];
	t[5] = statechainv[4] ^ statechainv[4 + 8] ^ statechainv[4 + 16] ^ statechainv[4 + 24] ^ statechainv[4 + 32];
	t[4] = statechainv[3] ^ statechainv[3 + 8] ^ statechainv[3 + 16] ^ statechainv[3 + 24] ^ statechainv[3 + 32] ^ tmp;
	t[3] = statechainv[2] ^ statechainv[2 + 8] ^ statechainv[2 + 16] ^ statechainv[2 + 24] ^ statechainv[2 + 32] ^ tmp;
	t[2] = statechainv[1] ^ statechainv[1 + 8] ^ statechainv[1 + 16] ^ statechainv[1 + 24] ^ statechainv[1 + 32];
	t[1] = statechainv[0] ^ statechainv[0 + 8] ^ statechainv[0 + 16] ^ statechainv[0 + 24] ^ statechainv[0 + 32] ^ tmp;
	t[0] = tmp;

	//	*(uint2x4*)statechainv ^= *(uint2x4*)t;
#pragma unroll 8
	for (int i = 0; i < 8; i++)
		statechainv[i] ^= t[i];

#pragma unroll 4
	for (int j = 1; j < 5; j++) {
#pragma unroll 8
		for (int i = 0; i < 8; i++)
			statechainv[i + (j << 3)] ^= t[i];
		//		*(uint2x4*)&statechainv[8*j] ^= *(uint2x4*)t;
#pragma unroll 8
		for (int i = 0; i < 8; i++)
			t[i + (j << 3)] = statechainv[i + (j << 3)];
		//		*(uint2x4*)&t[8*j] = *(uint2x4*)&statechainv[8*j];
	}

	//	*(uint2x4*)t = *(uint2x4*)statechainv;
#pragma unroll 8
	for (int i = 0; i < 8; i++)
		t[i] = statechainv[i];

	MULT0(statechainv);

#pragma unroll 4
	for (int j = 1; j < 5; j++)
		MULT2(statechainv, j);

#pragma unroll 5
	for (int j = 0; j < 5; j++)
#pragma unroll 8
		for (int i = 0; i < 8; i++)
			statechainv[i + 8 * j] ^= t[i + (8 * ((j + 1) % 5))];
	//		*(uint2x4*)&statechainv[8*j] ^= *(uint2x4*)&t[8*((j+1)%5)];

#pragma unroll 5
	for (int j = 0; j < 5; j++)
		*(uint2x4*)&t[8 * j] = *(uint2x4*)&statechainv[8 * j];

	MULT0(statechainv);
#pragma unroll 4
	for (int j = 1; j < 5; j++)
		MULT2(statechainv, j);

#pragma unroll 5
	for (int j = 0; j < 5; j++)
		*(uint2x4*)&statechainv[8 * j] ^= *(uint2x4*)&t[8 * ((j + 4) % 5)];

#pragma unroll 5
	for (int j = 0; j < 5; j++) {
		*(uint2x4*)&statechainv[8 * j] ^= *(uint2x4*)statebuffer;
		MULT0(statebuffer);
	}

	TWEAK(statechainv[12], statechainv[13], statechainv[14], statechainv[15], 1);
	TWEAK(statechainv[20], statechainv[21], statechainv[22], statechainv[23], 2);
	TWEAK(statechainv[28], statechainv[29], statechainv[30], statechainv[31], 3);
	TWEAK(statechainv[36], statechainv[37], statechainv[38], statechainv[39], 4);

	for (int i = 0; i < 8; i++){
		STEP2(statechainv, *(uint2*)&c_CNS[(2 * i) + 0], *(uint2*)&c_CNS[(2 * i) + 16]);
		STEP2(&statechainv[16], *(uint2*)&c_CNS[(2 * i) + 32], *(uint2*)&c_CNS[(2 * i) + 48]);
		STEP1(&statechainv[32], *(uint2*)&c_CNS[(2 * i) + 64]);
	}
}

__device__
static void rnd512_first(uint32_t *const __restrict__ state, uint32_t *const __restrict__ buffer)
{
#pragma unroll 5
	for (int j = 0; j < 5; j++) {
		uint32_t tmp;
#pragma unroll 8
		for (int i = 0; i < 8; i++)
			state[i + (j << 3)] ^= buffer[i];
		MULT0(buffer);
	}
	TWEAK(state[12], state[13], state[14], state[15], 1);
	TWEAK(state[20], state[21], state[22], state[23], 2);
	TWEAK(state[28], state[29], state[30], state[31], 3);
	TWEAK(state[36], state[37], state[38], state[39], 4);

	for (int i = 0; i < 8; i++) {
		STEP2(&state[0], *(uint2*)&c_CNS[(2 * i) + 0], *(uint2*)&c_CNS[(2 * i) + 16]);
		STEP2(&state[16], *(uint2*)&c_CNS[(2 * i) + 32], *(uint2*)&c_CNS[(2 * i) + 48]);
		STEP1(&state[32], *(uint2*)&c_CNS[(2 * i) + 64]);
	}
}

__device__
static void qubit_rnd512_first(uint32_t *const __restrict__ statebuffer, uint32_t *const __restrict__ statechainv)
{

	*(uint4*)&statechainv[0] ^= *(uint4*)&statebuffer[0];
	statechainv[4] ^= statebuffer[4];

	*(uint4*)&statechainv[9] ^= *(uint4*)&statebuffer[0];
	statechainv[13] ^= statebuffer[4];

	*(uint4*)&statechainv[18] ^= *(uint4*)&statebuffer[0];
	statechainv[22] ^= statebuffer[4];

	*(uint4*)&statechainv[27] ^= *(uint4*)&statebuffer[0];
	statechainv[31] ^= statebuffer[4];

	statechainv[0 + 8 * 4] ^= statebuffer[4];
	statechainv[1 + 8 * 4] ^= statebuffer[4];
	statechainv[3 + 8 * 4] ^= statebuffer[4];
	statechainv[4 + 8 * 4] ^= statebuffer[4];
	*(uint4*)&statechainv[4 + 8 * 4] ^= *(uint4*)&statebuffer[0];

	TWEAK(statechainv[12], statechainv[13], statechainv[14], statechainv[15], 1);
	TWEAK(statechainv[20], statechainv[21], statechainv[22], statechainv[23], 2);
	TWEAK(statechainv[28], statechainv[29], statechainv[30], statechainv[31], 3);
	TWEAK(statechainv[36], statechainv[37], statechainv[38], statechainv[39], 4);

#pragma unroll 8
	for (uint32_t i = 0; i < 8; i++){
		STEP2(&statechainv[0], *(uint2*)&c_CNS[(2 * i) + 0], *(uint2*)&c_CNS[(2 * i) + 16]);
		STEP2(&statechainv[16], *(uint2*)&c_CNS[(2 * i) + 32], *(uint2*)&c_CNS[(2 * i) + 48]);
		STEP1(&statechainv[32], *(uint2*)&c_CNS[(2 * i) + 64]);
	}
}


__device__ __forceinline__
static void rnd512_nullhash(uint32_t *const __restrict__ state){

	uint32_t t[40];
	uint32_t tmp;

	tmp = state[7] ^ state[7 + 8] ^ state[7 + 16] ^ state[7 + 24] ^ state[7 + 32];
	t[7] = state[6] ^ state[6 + 8] ^ state[6 + 16] ^ state[6 + 24] ^ state[6 + 32];
	t[6] = state[5] ^ state[5 + 8] ^ state[5 + 16] ^ state[5 + 24] ^ state[5 + 32];
	t[5] = state[4] ^ state[4 + 8] ^ state[4 + 16] ^ state[4 + 24] ^ state[4 + 32];
	t[4] = state[3] ^ state[3 + 8] ^ state[3 + 16] ^ state[3 + 24] ^ state[3 + 32] ^ tmp;
	t[3] = state[2] ^ state[2 + 8] ^ state[2 + 16] ^ state[2 + 24] ^ state[2 + 32] ^ tmp;
	t[2] = state[1] ^ state[1 + 8] ^ state[1 + 16] ^ state[1 + 24] ^ state[1 + 32];
	t[1] = state[0] ^ state[0 + 8] ^ state[0 + 16] ^ state[0 + 24] ^ state[0 + 32] ^ tmp;
	t[0] = tmp;

#pragma unroll 5
	for (int j = 0; j < 5; j++){
		//		#pragma unroll 8
		//		for(int i=0;i<8;i++)
		//			state[i+(j<<3)] ^= t[i];
		*(uint2x4*)&state[8 * j] ^= *(uint2x4*)t;
	}

#pragma unroll 5
	for (int j = 0; j < 5; j++){
		///		#pragma unroll 8
		///		for(int i=0;i<8;i++)
		//			t[i+(j<<3)] = state[i+(j<<3)];
		*(uint2x4*)&t[8 * j] = *(uint2x4*)&state[8 * j];
	}
#pragma unroll 5
	for (int j = 0; j < 5; j++) {
		MULT2(state, j);
	}

#pragma unroll 5
	for (int j = 0; j < 5; j++) {
		//		#pragma unroll 8
		//		for(int i=0;i<8;i++)
		//			state[i+(j<<3)] ^= t[i + (((j + 1) % 5)<<3)];
		*(uint2x4*)&state[8 * j] ^= *(uint2x4*)&t[8 * ((j + 1) % 5)];
	}

#pragma unroll 5
	for (int j = 0; j < 5; j++) {
		//		#pragma unroll 8
		//		for(int i=0;i<8;i++)
		//			t[i+8*j] = state[i+8*j];
		*(uint2x4*)&t[8 * j] = *(uint2x4*)&state[8 * j];
	}

#pragma unroll 5
	for (int j = 0; j < 5; j++) {
		MULT2(state, j);
	}

#pragma unroll 5
	for (int j = 0; j < 5; j++) {
#pragma unroll 8
		for (int i = 0; i < 8; i++)
			state[i + 8 * j] ^= t[i + (8 * ((j + 4) % 5))];
		//		*(uint2x4*)&state[8*j] ^= *(uint2x4*)&t[8 * ((j + 4) % 5)];
	}

	TWEAK(state[12], state[13], state[14], state[15], 1);
	TWEAK(state[20], state[21], state[22], state[23], 2);
	TWEAK(state[28], state[29], state[30], state[31], 3);
	TWEAK(state[36], state[37], state[38], state[39], 4);

	//	#pragma unroll 8
	for (int i = 0; i < 8; i++) {
		STEP2(&state[0], *(uint2*)&c_CNS[(2 * i) + 0], *(uint2*)&c_CNS[(2 * i) + 16]);
		STEP2(&state[16], *(uint2*)&c_CNS[(2 * i) + 32], *(uint2*)&c_CNS[(2 * i) + 48]);
		STEP1(&state[32], *(uint2*)&c_CNS[(2 * i) + 64]);
	}
}




//SHA512 MACROS ---------------------------------------------------------------------------

#define SWAP64(u64) cuda_swab64(u64)

static __constant__ uint64_t c_WB[80] = {
	0x428A2F98D728AE22, 0x7137449123EF65CD, 0xB5C0FBCFEC4D3B2F, 0xE9B5DBA58189DBBC,
	0x3956C25BF348B538, 0x59F111F1B605D019, 0x923F82A4AF194F9B, 0xAB1C5ED5DA6D8118,
	0xD807AA98A3030242, 0x12835B0145706FBE, 0x243185BE4EE4B28C, 0x550C7DC3D5FFB4E2,
	0x72BE5D74F27B896F, 0x80DEB1FE3B1696B1, 0x9BDC06A725C71235, 0xC19BF174CF692694,
	0xE49B69C19EF14AD2, 0xEFBE4786384F25E3, 0x0FC19DC68B8CD5B5, 0x240CA1CC77AC9C65,
	0x2DE92C6F592B0275, 0x4A7484AA6EA6E483, 0x5CB0A9DCBD41FBD4, 0x76F988DA831153B5,
	0x983E5152EE66DFAB, 0xA831C66D2DB43210, 0xB00327C898FB213F, 0xBF597FC7BEEF0EE4,
	0xC6E00BF33DA88FC2, 0xD5A79147930AA725, 0x06CA6351E003826F, 0x142929670A0E6E70,
	0x27B70A8546D22FFC, 0x2E1B21385C26C926, 0x4D2C6DFC5AC42AED, 0x53380D139D95B3DF,
	0x650A73548BAF63DE, 0x766A0ABB3C77B2A8, 0x81C2C92E47EDAEE6, 0x92722C851482353B,
	0xA2BFE8A14CF10364, 0xA81A664BBC423001, 0xC24B8B70D0F89791, 0xC76C51A30654BE30,
	0xD192E819D6EF5218, 0xD69906245565A910, 0xF40E35855771202A, 0x106AA07032BBD1B8,
	0x19A4C116B8D2D0C8, 0x1E376C085141AB53, 0x2748774CDF8EEB99, 0x34B0BCB5E19B48A8,
	0x391C0CB3C5C95A63, 0x4ED8AA4AE3418ACB, 0x5B9CCA4F7763E373, 0x682E6FF3D6B2B8A3,
	0x748F82EE5DEFB2FC, 0x78A5636F43172F60, 0x84C87814A1F0AB72, 0x8CC702081A6439EC,
	0x90BEFFFA23631E28, 0xA4506CEBDE82BDE9, 0xBEF9A3F7B2C67915, 0xC67178F2E372532B,
	0xCA273ECEEA26619C, 0xD186B8C721C0C207, 0xEADA7DD6CDE0EB1E, 0xF57D4F7FEE6ED178,
	0x06F067AA72176FBA, 0x0A637DC5A2C898A6, 0x113F9804BEF90DAE, 0x1B710B35131C471B,
	0x28DB77F523047D84, 0x32CAAB7B40C72493, 0x3C9EBE0A15C9BEBC, 0x431D67C49C100D4C,
	0x4CC5D4BECB3E42B6, 0x597F299CFC657E2A, 0x5FCB6FAB3AD6FAEC, 0x6C44198C4A475817
};

#define ROTR64_c(x, n)  (((x) >> (n)) | ((x) << (64 - (n))))

#define BSG5_0(x) xor3(ROTR64(x,28), ROTR64(x,34), ROTR64(x,39))
#define SSG5_0(x) xor3(ROTR64(x, 1), ROTR64(x ,8), shr_u64(x,7))
#define SSG5_1(x) xor3(ROTR64(x,19), ROTR64(x,61), shr_u64(x,6))


#define BSG5_0_c(x) ((ROTR64_c(x,28)^ ROTR64_c(x,34)^ ROTR64_c(x,39))
#define SSG5_0_c(x) ((ROTR64_c(x, 1)^ ROTR64_c(x ,8)^ x>>7))
#define SSG5_1_c(x) ((ROTR64_c(x,19)^ ROTR64_c(x,61)^ x>>6))


#define MAJ(X, Y, Z)   (((X) & (Y)) | (((X) | (Y)) & (Z)))
//#define MAJ(x, y, z)   andor(x,y,z)

__device__ __forceinline__
uint64_t Tone(const uint64_t* K, uint64_t* r, uint64_t* W, const uint8_t a, const uint8_t i)
{
	//asm("// TONE \n");
	const uint64_t e = r[(a + 4) & 7];
	const uint64_t BSG51 = xor3(ROTR64(e, 14), ROTR64(e, 18), ROTR64(e, 41));
	const uint64_t f = r[(a + 5) & 7];
	const uint64_t g = r[(a + 6) & 7];
	const uint64_t CHl = ((f ^ g) & e) ^ g; // xandx(e, f, g);
	return (r[(a + 7) & 7] + W[i] + BSG51 + CHl + K[i]);
}

#define SHA3_STEP(K, r, W, ord, i) { \
	const int a = (8 - ord) & 7; \
	uint64_t T1 = Tone(K, r, W, a, i); \
	r[(a+3) & 7]+= T1; \
	r[(a+7) & 7] = T1 + (BSG5_0(r[a]) + MAJ(r[a], r[(a+1) & 7], r[(a+2) & 7])); \
}

#define bsg5_0(x) (ROTR64(x,28) ^ ROTR64(x,34) ^ ROTR64(x,39))
#define bsg5_1(x) (ROTR64(x,14) ^ ROTR64(x,18) ^ ROTR64(x,41))
#define ssg5_0(x) (ROTR64(x, 1) ^ ROTR64(x, 8) ^ shr_u64(x,7))
#define ssg5_1(x) (ROTR64(x,19) ^ ROTR64(x,61) ^ shr_u64(x,6))


#define bsg5_0_c(x) (ROTR64_c(x,28) ^ ROTR64_c(x,34) ^ ROTR64_c(x,39))
#define bsg5_1_c(x) (ROTR64_c(x,14) ^ ROTR64_c(x,18) ^ ROTR64_c(x,41))

#define andor64(a,b,c) ((a & (b | c)) | (b & c))
#define xandx64(e,f,g) (g ^ (e & (g ^ f)))

static __constant__ const uint64_t K_512[80] = {
	0x428A2F98D728AE22, 0x7137449123EF65CD, 0xB5C0FBCFEC4D3B2F, 0xE9B5DBA58189DBBC, 0x3956C25BF348B538, 0x59F111F1B605D019, 0x923F82A4AF194F9B, 0xAB1C5ED5DA6D8118,
	0xD807AA98A3030242, 0x12835B0145706FBE, 0x243185BE4EE4B28C, 0x550C7DC3D5FFB4E2, 0x72BE5D74F27B896F, 0x80DEB1FE3B1696B1, 0x9BDC06A725C71235, 0xC19BF174CF692694,
	0xE49B69C19EF14AD2, 0xEFBE4786384F25E3, 0x0FC19DC68B8CD5B5, 0x240CA1CC77AC9C65, 0x2DE92C6F592B0275, 0x4A7484AA6EA6E483, 0x5CB0A9DCBD41FBD4, 0x76F988DA831153B5,
	0x983E5152EE66DFAB, 0xA831C66D2DB43210, 0xB00327C898FB213F, 0xBF597FC7BEEF0EE4, 0xC6E00BF33DA88FC2, 0xD5A79147930AA725, 0x06CA6351E003826F, 0x142929670A0E6E70,
	0x27B70A8546D22FFC, 0x2E1B21385C26C926, 0x4D2C6DFC5AC42AED, 0x53380D139D95B3DF, 0x650A73548BAF63DE, 0x766A0ABB3C77B2A8, 0x81C2C92E47EDAEE6, 0x92722C851482353B,
	0xA2BFE8A14CF10364, 0xA81A664BBC423001, 0xC24B8B70D0F89791, 0xC76C51A30654BE30, 0xD192E819D6EF5218, 0xD69906245565A910, 0xF40E35855771202A, 0x106AA07032BBD1B8,
	0x19A4C116B8D2D0C8, 0x1E376C085141AB53, 0x2748774CDF8EEB99, 0x34B0BCB5E19B48A8, 0x391C0CB3C5C95A63, 0x4ED8AA4AE3418ACB, 0x5B9CCA4F7763E373, 0x682E6FF3D6B2B8A3,
	0x748F82EE5DEFB2FC, 0x78A5636F43172F60, 0x84C87814A1F0AB72, 0x8CC702081A6439EC, 0x90BEFFFA23631E28, 0xA4506CEBDE82BDE9, 0xBEF9A3F7B2C67915, 0xC67178F2E372532B,
	0xCA273ECEEA26619C, 0xD186B8C721C0C207, 0xEADA7DD6CDE0EB1E, 0xF57D4F7FEE6ED178, 0x06F067AA72176FBA, 0x0A637DC5A2C898A6, 0x113F9804BEF90DAE, 0x1B710B35131C471B,
	0x28DB77F523047D84, 0x32CAAB7B40C72493, 0x3C9EBE0A15C9BEBC, 0x431D67C49C100D4C, 0x4CC5D4BECB3E42B6, 0x597F299CFC657E2A, 0x5FCB6FAB3AD6FAEC, 0x6C44198C4A475817
};
//--------------------- end SHA512 macros -----------------------
//keccak


#define U32TO64_LE(p) \
	(((uint64_t)(*p)) | (((uint64_t)(*(p + 1))) << 32))

#define U64TO32_LE(p, v) \
	*p = (uint32_t)((v)); *(p+1) = (uint32_t)((v) >> 32);

static const uint64_t host_keccak_round_constants[24] = {
	0x0000000000000001ull, 0x0000000000008082ull,
	0x800000000000808aull, 0x8000000080008000ull,
	0x000000000000808bull, 0x0000000080000001ull,
	0x8000000080008081ull, 0x8000000000008009ull,
	0x000000000000008aull, 0x0000000000000088ull,
	0x0000000080008009ull, 0x000000008000000aull,
	0x000000008000808bull, 0x800000000000008bull,
	0x8000000000008089ull, 0x8000000000008003ull,
	0x8000000000008002ull, 0x8000000000000080ull,
	0x000000000000800aull, 0x800000008000000aull,
	0x8000000080008081ull, 0x8000000000008080ull,
	0x0000000080000001ull, 0x8000000080008008ull
};

__constant__ uint64_t d_keccak_round_constants[24];

__constant__
uint2 keccak_round_constants[24] = {
	{ 0x00000001, 0x00000000 }, { 0x00008082, 0x00000000 }, { 0x0000808a, 0x80000000 }, { 0x80008000, 0x80000000 },
	{ 0x0000808b, 0x00000000 }, { 0x80000001, 0x00000000 }, { 0x80008081, 0x80000000 }, { 0x00008009, 0x80000000 },
	{ 0x0000008a, 0x00000000 }, { 0x00000088, 0x00000000 }, { 0x80008009, 0x00000000 }, { 0x8000000a, 0x00000000 },
	{ 0x8000808b, 0x00000000 }, { 0x0000008b, 0x80000000 }, { 0x00008089, 0x80000000 }, { 0x00008003, 0x80000000 },
	{ 0x00008002, 0x80000000 }, { 0x00000080, 0x80000000 }, { 0x0000800a, 0x00000000 }, { 0x8000000a, 0x80000000 },
	{ 0x80008081, 0x80000000 }, { 0x00008080, 0x80000000 }, { 0x80000001, 0x00000000 }, { 0x80008008, 0x80000000 }
};

//---------------------- en keccak


__global__ void __launch_bounds__(256, 5) tiger192_gpu_hash_64(int threads, uint32_t *d_hash)
{
	__shared__ uint64_t sharedMem[1024];
	//	if(threadIdx.x < 256)
	{
		sharedMem[threadIdx.x] = T1[threadIdx.x];
		sharedMem[threadIdx.x + 256] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 512] = T3[threadIdx.x];
		sharedMem[threadIdx.x+768]  = T4[threadIdx.x];
	}
	__syncthreads();

	int thread = (blockDim.x * blockIdx.x + threadIdx.x);
	if (thread < threads) {
		uint64_t* inout = (uint64_t*)&d_hash[thread << 4];
		uint64_t buf[3], in[8], in2[8];

#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = inout[i];

#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

		TIGER_ROUND_BODY(in, buf);

		in2[0] = 1;
#pragma unroll
		for (int i = 1; i < 7; i++) in2[i] = 0;
		in2[7] = 0x200;
		TIGER_ROUND_BODY(in2, buf);

		inout[0] = buf[0];
		inout[1] = buf[1];
		inout[2] = buf[2];
		inout[3] = 0;
		inout[4] = 0;
		inout[5] = 0;
		inout[6] = 0;
		inout[7] = 0;
	}
}
__global__ void __launch_bounds__(256, 5) tiger192_gpu_hash_64_rtx(int threads, uint32_t *d_hash)
{
	__shared__ uint64_t sharedMem[1024][4];
	//	if(threadIdx.x < 256)
	{
		sharedMem[threadIdx.x][0] = T1[threadIdx.x];
		sharedMem[threadIdx.x][1] = T1[threadIdx.x];
		sharedMem[threadIdx.x][2] = T1[threadIdx.x];
		sharedMem[threadIdx.x][3] = T1[threadIdx.x];
		sharedMem[threadIdx.x + 256][0] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][1] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][2] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][3] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 512][0] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][1] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][2] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][3] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 768][0] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][1] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][2] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][3] = T4[threadIdx.x];
	}
	__syncthreads();

	int thread = (blockDim.x * blockIdx.x + threadIdx.x);
	if (thread < threads) 
	{
		int index = threadIdx.x & 3;
		uint64_t* inout = (uint64_t*)&d_hash[thread << 4];
		uint64_t buf[3], in[8], in2[8];

#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = inout[i];

#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

		TIGER_ROUND_BODY_RTX(in, buf);

		in2[0] = 1;
#pragma unroll
		for (int i = 1; i < 7; i++) in2[i] = 0;
		in2[7] = 0x200;
		TIGER_ROUND_BODY_RTX(in2, buf);

		inout[0] = buf[0];
		inout[1] = buf[1];
		inout[2] = buf[2];
		inout[3] = 0;
		inout[4] = 0;
		inout[5] = 0;
		inout[6] = 0;
		inout[7] = 0;
	}
}

__global__ void __launch_bounds__(512, 2) tiger192Luffa512_gpu_hash_64_rtx(int threads, uint32_t *d_hash)
{
	__shared__ uint64_t sharedMem[1024][4];

	if (threadIdx.x < 256)
	{
		sharedMem[threadIdx.x][0] = T1[threadIdx.x];
		sharedMem[threadIdx.x][1] = T1[threadIdx.x];
		sharedMem[threadIdx.x][2] = T1[threadIdx.x];
		sharedMem[threadIdx.x][3] = T1[threadIdx.x];
		sharedMem[threadIdx.x + 256][0] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][1] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][2] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][3] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 512][0] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][1] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][2] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][3] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 768][0] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][1] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][2] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][3] = T4[threadIdx.x];
	}
	__syncthreads();

	int thread = (blockDim.x * blockIdx.x + threadIdx.x);
	if (thread < threads)
	{
		int index = threadIdx.x & 3;

		uint64_t* inout = (uint64_t*)&d_hash[thread << 4];
		uint64_t buf[3], in[8], in2[8];

#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = inout[i];

#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

		TIGER_ROUND_BODY_RTX(in, buf);

		in2[0] = 1;
#pragma unroll
		for (int i = 1; i < 7; i++) in2[i] = 0;
		in2[7] = 0x200;
		TIGER_ROUND_BODY_RTX(in2, buf);

		/*		uint64_t Hash[8];
		Hash[0] = buf[0];
		Hash[1] = buf[1];
		Hash[2] = buf[2];
		Hash[3] = 0;
		Hash[4] = 0;
		Hash[5] = 0;
		Hash[6] = 0;
		Hash[7] = 0;
		*/
		uint32_t statebuffer[8];

		uint32_t statechainv[40] = {
			0x8bb0a761, 0xc2e4aa8b, 0x2d539bc9, 0x381408f8, 0x478f6633, 0x255a46ff, 0x581c37f7, 0x601c2e8e,
			0x266c5f9d, 0xc34715d8, 0x8900670e, 0x51a540be, 0xe4ce69fb, 0x5089f4d4, 0x3cc0a506, 0x609bcb02,
			0xa4e3cd82, 0xd24fd6ca, 0xc0f196dc, 0xcf41eafe, 0x0ff2e673, 0x303804f2, 0xa7b3cd48, 0x677addd4,
			0x66e66a8a, 0x2303208f, 0x486dafb4, 0xc0d37dc6, 0x634d15af, 0xe5af6747, 0x10af7e38, 0xee7e6428,
			0x01262e5d, 0xc92c2e64, 0x82fee966, 0xcea738d3, 0x867de2b0, 0xe0714818, 0xda6e831f, 0xa7062529
		};
		//			uint2x4* Hash = (uint2x4*)&g_hash[thread << 4];

		uint32_t hash[16];
		uint64_t *peker = (uint64_t *)&hash[0];

		peker[0] = buf[0];
		peker[1] = buf[1];
		peker[2] = buf[2];
		peker[3] = 0;
		peker[4] = 0;
		peker[5] = 0;
		peker[6] = 0;
		peker[7] = 0;


		//			*(uint2x4*)&hash[0] = *(uint2x4*)&Hash[0];
		//			*(uint2x4*)&hash[8] = *(uint2x4*)&Hash[4];

#pragma unroll 8
		for (int i = 0; i < 8; i++){
			statebuffer[i] = cuda_swab32(hash[i]);
		}

		rnd512_first(statechainv, statebuffer);

#pragma unroll 8
		for (int i = 0; i < 8; i++){
			statebuffer[i] = cuda_swab32(hash[8 + i]);
		}

		rnd512(statebuffer, statechainv);

		statebuffer[0] = 0x80000000;
#pragma unroll 7
		for (uint32_t i = 1; i < 8; i++)
			statebuffer[i] = 0;

		rnd512(statebuffer, statechainv);

		/*---- blank round with m=0 ----*/
		rnd512_nullhash(statechainv);

#pragma unroll 8
		for (int i = 0; i < 8; i++)
			hash[i] = cuda_swab32(statechainv[i] ^ statechainv[i + 8] ^ statechainv[i + 16] ^ statechainv[i + 24] ^ statechainv[i + 32]);

		rnd512_nullhash(statechainv);

#pragma unroll 8
		for (int i = 0; i < 8; i++)
			hash[8 + i] = cuda_swab32(statechainv[i] ^ statechainv[i + 8] ^ statechainv[i + 16] ^ statechainv[i + 24] ^ statechainv[i + 32]);

		//*(uint2x4*)&inout[0] = *(uint2x4*)&hash[0];
		//*(uint2x4*)&inout[1] = *(uint2x4*)&hash[8];
		inout[0] = peker[0];
		inout[1] = peker[1];
		inout[2] = peker[2];
		inout[3] = peker[3];
		inout[4] = peker[4];
		inout[5] = peker[5];
		inout[6] = peker[6];
		inout[7] = peker[7];

	}
}

__global__ void __launch_bounds__(512, 2) tiger192Luffa512_gpu_hash_64_rtx_final(int threads, uint32_t *d_hash, uint32_t* resNonce, uint64_t target)
{
	__shared__ uint64_t sharedMem[1024][4];

	if (threadIdx.x < 256)
	{
		sharedMem[threadIdx.x][0] = T1[threadIdx.x];
		sharedMem[threadIdx.x][1] = T1[threadIdx.x];
		sharedMem[threadIdx.x][2] = T1[threadIdx.x];
		sharedMem[threadIdx.x][3] = T1[threadIdx.x];
		sharedMem[threadIdx.x + 256][0] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][1] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][2] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][3] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 512][0] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][1] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][2] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][3] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 768][0] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][1] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][2] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][3] = T4[threadIdx.x];
	}
	__syncthreads();

	int thread = (blockDim.x * blockIdx.x + threadIdx.x);
	if (thread < threads)
	{
		int index = threadIdx.x & 3;

		uint64_t* inout = (uint64_t*)&d_hash[thread << 4];
		uint64_t buf[3], in[8], in2[8];

#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = inout[i];

#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

		TIGER_ROUND_BODY_RTX(in, buf);

		in2[0] = 1;
#pragma unroll
		for (int i = 1; i < 7; i++) in2[i] = 0;
		in2[7] = 0x200;
		TIGER_ROUND_BODY_RTX(in2, buf);

		/*		uint64_t Hash[8];
				Hash[0] = buf[0];
				Hash[1] = buf[1];
				Hash[2] = buf[2];
				Hash[3] = 0;
				Hash[4] = 0;
				Hash[5] = 0;
				Hash[6] = 0;
				Hash[7] = 0;
				*/
		uint32_t statebuffer[8];

		uint32_t statechainv[40] = {
			0x8bb0a761, 0xc2e4aa8b, 0x2d539bc9, 0x381408f8, 0x478f6633, 0x255a46ff, 0x581c37f7, 0x601c2e8e,
			0x266c5f9d, 0xc34715d8, 0x8900670e, 0x51a540be, 0xe4ce69fb, 0x5089f4d4, 0x3cc0a506, 0x609bcb02,
			0xa4e3cd82, 0xd24fd6ca, 0xc0f196dc, 0xcf41eafe, 0x0ff2e673, 0x303804f2, 0xa7b3cd48, 0x677addd4,
			0x66e66a8a, 0x2303208f, 0x486dafb4, 0xc0d37dc6, 0x634d15af, 0xe5af6747, 0x10af7e38, 0xee7e6428,
			0x01262e5d, 0xc92c2e64, 0x82fee966, 0xcea738d3, 0x867de2b0, 0xe0714818, 0xda6e831f, 0xa7062529
		};
		//			uint2x4* Hash = (uint2x4*)&g_hash[thread << 4];

		uint32_t hash[16];
		uint64_t *peker = (uint64_t *)&hash[0];

		peker[0] = buf[0];
		peker[1] = buf[1];
		peker[2] = buf[2];
		peker[3] = 0;
		peker[4] = 0;
		peker[5] = 0;
		peker[6] = 0;
		peker[7] = 0;


		//			*(uint2x4*)&hash[0] = *(uint2x4*)&Hash[0];
		//			*(uint2x4*)&hash[8] = *(uint2x4*)&Hash[4];

#pragma unroll 8
		for (int i = 0; i < 8; i++){
			statebuffer[i] = cuda_swab32(hash[i]);
		}

		rnd512_first(statechainv, statebuffer);

#pragma unroll 8
		for (int i = 0; i < 8; i++){
			statebuffer[i] = cuda_swab32(hash[8 + i]);
		}

		rnd512(statebuffer, statechainv);

		statebuffer[0] = 0x80000000;
#pragma unroll 7
		for (uint32_t i = 1; i < 8; i++)
			statebuffer[i] = 0;

		rnd512(statebuffer, statechainv);

		/*---- blank round with m=0 ----*/
		rnd512_nullhash(statechainv);

#pragma unroll 8
		for (int i = 0; i < 8; i++)
			hash[i] = cuda_swab32(statechainv[i] ^ statechainv[i + 8] ^ statechainv[i + 16] ^ statechainv[i + 24] ^ statechainv[i + 32]);

		//		rnd512_nullhash(statechainv);
		//		#pragma unroll 8
		//		for (int i = 0; i<8; i++)
		//			hash[8 + i] = (statechainv[i] ^ statechainv[i + 8] ^ statechainv[i + 16] ^ statechainv[i + 24] ^ statechainv[i + 32]);

		//		Hash[0] = *(uint2x4*)&hash[0];
		//		Hash[1] = *(uint2x4*)&hash[8];

		if (devectorize(make_uint2(hash[6], hash[7])) <= target)
		{
			const uint32_t tmp = atomicExch(&resNonce[0], thread);
			if (tmp != UINT32_MAX)
				resNonce[1] = tmp;
		}
	}
}

__global__ void __launch_bounds__(512, 2) tiger192keccak512_gpu_hash_64_rtx_final(int threads, uint32_t *d_hash, uint32_t* resNonce, uint64_t target)
{
	__shared__ uint64_t sharedMem[1024][4];

	if (threadIdx.x < 256)
	{
		sharedMem[threadIdx.x][0] = T1[threadIdx.x];
		sharedMem[threadIdx.x][1] = T1[threadIdx.x];
		sharedMem[threadIdx.x][2] = T1[threadIdx.x];
		sharedMem[threadIdx.x][3] = T1[threadIdx.x];
		sharedMem[threadIdx.x + 256][0] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][1] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][2] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][3] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 512][0] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][1] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][2] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][3] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 768][0] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][1] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][2] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][3] = T4[threadIdx.x];
	}
	__syncthreads();

	int thread = (blockDim.x * blockIdx.x + threadIdx.x);
	if (thread < threads)
	{
		int index = threadIdx.x & 3;

		uint64_t* inout = (uint64_t*)&d_hash[thread << 4];
		uint64_t buf[3], in[8], in2[8];

#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = inout[i];

#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

		TIGER_ROUND_BODY_RTX(in, buf);

		in2[0] = 1;
#pragma unroll
		for (int i = 1; i < 7; i++) in2[i] = 0;
		in2[7] = 0x200;
		TIGER_ROUND_BODY_RTX(in2, buf);

/*		uint32_t hashPosition = thread;
		uint2x4* d_hash = (uint2x4 *)&g_hash[hashPosition * 8];

#if __CUDA_ARCH__ > 500
		* (uint2x4*)&s[0] = __ldg4(&d_hash[0]);
		*(uint2x4*)&s[4] = __ldg4(&d_hash[1]);
#else
		*(uint2x4*)&s[0] = d_hash[0];
		*(uint2x4*)&s[4] = d_hash[1];
#endif
		*/	
		const uint32_t hashPosition = thread;

		uint2 t[5], u[5], v, w;
		uint2 s[25];

		const uint2 keccak_round_constants2[24] = {
			{ 0x00000001, 0x00000000 }, { 0x00008082, 0x00000000 }, { 0x0000808a, 0x80000000 }, { 0x80008000, 0x80000000 },
			{ 0x0000808b, 0x00000000 }, { 0x80000001, 0x00000000 }, { 0x80008081, 0x80000000 }, { 0x00008009, 0x80000000 },
			{ 0x0000008a, 0x00000000 }, { 0x00000088, 0x00000000 }, { 0x80008009, 0x00000000 }, { 0x8000000a, 0x00000000 },
			{ 0x8000808b, 0x00000000 }, { 0x0000008b, 0x80000000 }, { 0x00008089, 0x80000000 }, { 0x00008003, 0x80000000 },
			{ 0x00008002, 0x80000000 }, { 0x00000080, 0x80000000 }, { 0x0000800a, 0x00000000 }, { 0x8000000a, 0x80000000 },
			{ 0x80008081, 0x80000000 }, { 0x00008080, 0x80000000 }, { 0x80000001, 0x00000000 }, { 0x80008008, 0x80000000 }
		};


		s[0] = vectorize(buf[0]);
		s[1] = vectorize(buf[1]);
		s[2] = vectorize(buf[2]);
		s[3].x = 0;
		s[3].y = 0;

		s[4].x = 0;
		s[4].y = 0;
		s[5].x = 0;
		s[5].y = 0;
		s[6].x = 0;
		s[6].y = 0;
		s[7].x = 0;
		s[7].y = 0;


		s[8] = make_uint2(1, 0x80000000);

		/*theta*/
		t[0] = vectorize(devectorize(s[0]) ^ devectorize(s[5]));
		t[1] = vectorize(devectorize(s[1]) ^ devectorize(s[6]));
		t[2] = vectorize(devectorize(s[2]) ^ devectorize(s[7]));
		t[3] = vectorize(devectorize(s[3]) ^ devectorize(s[8]));
		t[4] = s[4];

		/*theta*/
#pragma unroll 5
		for (int j = 0; j<5; j++){
			u[j] = ROL2(t[j], 1);
		}

		s[4] = xor3x(s[4], t[3], u[0]);
		s[24] = s[19] = s[14] = s[9] = t[3] ^ u[0];

		s[0] = xor3x(s[0], t[4], u[1]);
		s[5] = xor3x(s[5], t[4], u[1]);
		s[20] = s[15] = s[10] = t[4] ^ u[1];

		s[1] = xor3x(s[1], t[0], u[2]);
		s[6] = xor3x(s[6], t[0], u[2]);
		s[21] = s[16] = s[11] = t[0] ^ u[2];

		s[2] = xor3x(s[2], t[1], u[3]);
		s[7] = xor3x(s[7], t[1], u[3]);
		s[22] = s[17] = s[12] = t[1] ^ u[3];

		s[3] = xor3x(s[3], t[2], u[4]); s[8] = xor3x(s[8], t[2], u[4]);
		s[23] = s[18] = s[13] = t[2] ^ u[4];
		/* rho pi: b[..] = rotl(a[..], ..) */
		v = s[1];
		s[1] = ROL2(s[6], 44);
		s[6] = ROL2(s[9], 20);
		s[9] = ROL2(s[22], 61);
		s[22] = ROL2(s[14], 39);
		s[14] = ROL2(s[20], 18);
		s[20] = ROL2(s[2], 62);
		s[2] = ROL2(s[12], 43);
		s[12] = ROL2(s[13], 25);
		s[13] = ROL8(s[19]);
		s[19] = ROR8(s[23]);
		s[23] = ROL2(s[15], 41);
		s[15] = ROL2(s[4], 27);
		s[4] = ROL2(s[24], 14);
		s[24] = ROL2(s[21], 2);
		s[21] = ROL2(s[8], 55);
		s[8] = ROL2(s[16], 45);
		s[16] = ROL2(s[5], 36);
		s[5] = ROL2(s[3], 28);
		s[3] = ROL2(s[18], 21);
		s[18] = ROL2(s[17], 15);
		s[17] = ROL2(s[11], 10);
		s[11] = ROL2(s[7], 6);
		s[7] = ROL2(s[10], 3);
		s[10] = ROL2(v, 1);
		/* chi: a[i,j] ^= ~b[i,j+1] & b[i,j+2] */
#pragma unroll 5
		for (int j = 0; j<25; j += 5){
			v = s[j]; w = s[j + 1]; s[j] = chi(v, w, s[j + 2]); s[j + 1] = chi(w, s[j + 2], s[j + 3]); s[j + 2] = chi(s[j + 2], s[j + 3], s[j + 4]); s[j + 3] = chi(s[j + 3], s[j + 4], v); s[j + 4] = chi(s[j + 4], v, w);
		}
		/* iota: a[0,0] ^= round constant */
		s[0].x ^= 1;

		//#if __CUDA_ARCH__ > 500
		//#pragma unroll 4
		//#else
		//#pragma unroll 3
		//#endif
#pragma unroll
		for (int i = 1; i < 23; i++)
		{
			/*theta*/
#pragma unroll 5
			for (int j = 0; j<5; j++){
				t[j] = vectorize(xor5(devectorize(s[j]), devectorize(s[j + 5]), devectorize(s[j + 10]), devectorize(s[j + 15]), devectorize(s[j + 20])));
			}

			/*theta*/
#pragma unroll 5
			for (int j = 0; j<5; j++){
				u[j] = ROL2(t[j], 1);
			}
			s[4] = xor3x(s[4], t[3], u[0]); s[9] = xor3x(s[9], t[3], u[0]); s[14] = xor3x(s[14], t[3], u[0]); s[19] = xor3x(s[19], t[3], u[0]); s[24] = xor3x(s[24], t[3], u[0]);
			s[0] = xor3x(s[0], t[4], u[1]); s[5] = xor3x(s[5], t[4], u[1]); s[10] = xor3x(s[10], t[4], u[1]); s[15] = xor3x(s[15], t[4], u[1]); s[20] = xor3x(s[20], t[4], u[1]);
			s[1] = xor3x(s[1], t[0], u[2]); s[6] = xor3x(s[6], t[0], u[2]); s[11] = xor3x(s[11], t[0], u[2]); s[16] = xor3x(s[16], t[0], u[2]); s[21] = xor3x(s[21], t[0], u[2]);
			s[2] = xor3x(s[2], t[1], u[3]); s[7] = xor3x(s[7], t[1], u[3]); s[12] = xor3x(s[12], t[1], u[3]); s[17] = xor3x(s[17], t[1], u[3]); s[22] = xor3x(s[22], t[1], u[3]);
			s[3] = xor3x(s[3], t[2], u[4]); s[8] = xor3x(s[8], t[2], u[4]); s[13] = xor3x(s[13], t[2], u[4]); s[18] = xor3x(s[18], t[2], u[4]); s[23] = xor3x(s[23], t[2], u[4]);

			/* rho pi: b[..] = rotl(a[..], ..) */
			v = s[1];
			s[1] = ROL2(s[6], 44);
			s[6] = ROL2(s[9], 20);
			s[9] = ROL2(s[22], 61);
			s[22] = ROL2(s[14], 39);
			s[14] = ROL2(s[20], 18);
			s[20] = ROL2(s[2], 62);
			s[2] = ROL2(s[12], 43);
			s[12] = ROL2(s[13], 25);
			s[13] = ROL8(s[19]);
			s[19] = ROR8(s[23]);
			s[23] = ROL2(s[15], 41);
			s[15] = ROL2(s[4], 27);
			s[4] = ROL2(s[24], 14);
			s[24] = ROL2(s[21], 2);
			s[21] = ROL2(s[8], 55);
			s[8] = ROL2(s[16], 45);
			s[16] = ROL2(s[5], 36);
			s[5] = ROL2(s[3], 28);
			s[3] = ROL2(s[18], 21);
			s[18] = ROL2(s[17], 15);
			s[17] = ROL2(s[11], 10);
			s[11] = ROL2(s[7], 6);
			s[7] = ROL2(s[10], 3);
			s[10] = ROL2(v, 1);

			/* chi: a[i,j] ^= ~b[i,j+1] & b[i,j+2] */
#pragma unroll 5
			for (int j = 0; j<25; j += 5){
				v = s[j]; w = s[j + 1]; s[j] = chi(v, w, s[j + 2]); s[j + 1] = chi(w, s[j + 2], s[j + 3]); s[j + 2] = chi(s[j + 2], s[j + 3], s[j + 4]); s[j + 3] = chi(s[j + 3], s[j + 4], v); s[j + 4] = chi(s[j + 4], v, w);
			}

			/* iota: a[0,0] ^= round constant */
			s[0] ^= keccak_round_constants2[i];
		}
		/*theta*/
#pragma unroll 5
		for (int j = 0; j<5; j++){
			t[j] = xor3x(xor3x(s[j + 0], s[j + 5], s[j + 10]), s[j + 15], s[j + 20]);
		}
		/*theta*/
		u[0] = ROL2(t[0], 1);
		u[1] = ROL2(t[1], 1);
		s[18] = xor3x(s[18], t[2], ROL2(t[4], 1));
		s[24] = xor3x(s[24], t[3], u[0]);
		s[0] = xor3x(s[0], t[4], u[1]);
		/* rho pi: b[..] = rotl(a[..], ..) */
		s[3] = ROL2(s[18], 21);
		s[4] = ROL2(s[24], 14);
		/* chi: a[i,j] ^= ~b[i,j+1] & b[i,j+2] */
		if (devectorize(chi(s[3], s[4], s[0])) <= target)
		{
			const uint32_t tmp = atomicExch(&resNonce[0], hashPosition);
			if (tmp != UINT32_MAX)
				resNonce[1] = tmp;
		}
	}
}




__global__ void __launch_bounds__(256, 3) tiger192Keccak512_gpu_hash_64_rtx(int threads, uint32_t *d_hash)
{
	__shared__ uint64_t sharedMem[1024][4];
	if(threadIdx.x < 256)
	{
		sharedMem[threadIdx.x][0] = T1[threadIdx.x];
		sharedMem[threadIdx.x][1] = T1[threadIdx.x];
		sharedMem[threadIdx.x][2] = T1[threadIdx.x];
		sharedMem[threadIdx.x][3] = T1[threadIdx.x];
		sharedMem[threadIdx.x + 256][0] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][1] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][2] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][3] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 512][0] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][1] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][2] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][3] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 768][0] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][1] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][2] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][3] = T4[threadIdx.x];
	}
	__syncthreads();

	int thread = (blockDim.x * blockIdx.x + threadIdx.x);
	if (thread < threads)
	{
		int index = threadIdx.x & 3;
		uint64_t* inout = (uint64_t*)&d_hash[thread << 4];
		uint64_t buf[3], in[8], in2[8];

#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = inout[i];

#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

		TIGER_ROUND_BODY_RTX(in, buf);

		in2[0] = 1;
#pragma unroll
		for (int i = 1; i < 7; i++) in2[i] = 0;
		in2[7] = 0x200;
		TIGER_ROUND_BODY_RTX(in2, buf);

	//  inout[0] = buf[0];
	//	inout[1] = buf[1];
	//	inout[2] = buf[2];
	//	inout[3] = 0;
	//	inout[4] = 0;
	//	inout[5] = 0;
	//	inout[6] = 0;
	//	inout[7] = 0;

		uint2 t[5], u[5], v, w;
		uint2 s[25];

		s[0] = vectorize(buf[0]);
		s[1] = vectorize(buf[1]);
		s[2] = vectorize(buf[2]);
		s[3].x = 0;
		s[3].y = 0;

		s[4].x = 0;
		s[4].y = 0;
		s[5].x = 0;
		s[5].y = 0;
		s[6].x = 0;
		s[6].y = 0;
		s[7].x = 0;
		s[7].y = 0;


		const uint32_t hashPosition = thread;
//		uint2x4* d_hash = (uint2x4 *)&inout[hashPosition * 8];

//		#if __CUDA_ARCH__ > 500
//		* (uint2x4*)&s[0] = __ldg4(&d_hash[0]);
//		*(uint2x4*)&s[4] = __ldg4(&d_hash[1]);
//		#else
//		*(uint2x4*)&s[0] = d_hash[0];
//		*(uint2x4*)&s[4] = d_hash[1];
//		#endif

		s[8] = make_uint2(1, 0x80000000);

		/*theta*/
		t[0] = vectorize(devectorize(s[0]) ^ devectorize(s[5]));
		t[1] = vectorize(devectorize(s[1]) ^ devectorize(s[6]));
		t[2] = vectorize(devectorize(s[2]) ^ devectorize(s[7]));
		t[3] = vectorize(devectorize(s[3]) ^ devectorize(s[8]));
		t[4] = s[4];

		/*theta*/
#pragma unroll 5
		for (int j = 0; j<5; j++){
			u[j] = ROL2(t[j], 1);
		}

		s[4] = xor3x(s[4], t[3], u[0]);
		s[24] = s[19] = s[14] = s[9] = t[3] ^ u[0];

		s[0] = xor3x(s[0], t[4], u[1]);
		s[5] = xor3x(s[5], t[4], u[1]);
		s[20] = s[15] = s[10] = t[4] ^ u[1];

		s[1] = xor3x(s[1], t[0], u[2]);
		s[6] = xor3x(s[6], t[0], u[2]);
		s[21] = s[16] = s[11] = t[0] ^ u[2];

		s[2] = xor3x(s[2], t[1], u[3]);
		s[7] = xor3x(s[7], t[1], u[3]);
		s[22] = s[17] = s[12] = t[1] ^ u[3];

		s[3] = xor3x(s[3], t[2], u[4]); s[8] = xor3x(s[8], t[2], u[4]);
		s[23] = s[18] = s[13] = t[2] ^ u[4];
		/* rho pi: b[..] = rotl(a[..], ..) */
		v = s[1];
		s[1] = ROL2(s[6], 44);
		s[6] = ROL2(s[9], 20);
		s[9] = ROL2(s[22], 61);
		s[22] = ROL2(s[14], 39);
		s[14] = ROL2(s[20], 18);
		s[20] = ROL2(s[2], 62);
		s[2] = ROL2(s[12], 43);
		s[12] = ROL2(s[13], 25);
		s[13] = ROL8(s[19]);
		s[19] = ROR8(s[23]);
		s[23] = ROL2(s[15], 41);
		s[15] = ROL2(s[4], 27);
		s[4] = ROL2(s[24], 14);
		s[24] = ROL2(s[21], 2);
		s[21] = ROL2(s[8], 55);
		s[8] = ROL2(s[16], 45);
		s[16] = ROL2(s[5], 36);
		s[5] = ROL2(s[3], 28);
		s[3] = ROL2(s[18], 21);
		s[18] = ROL2(s[17], 15);
		s[17] = ROL2(s[11], 10);
		s[11] = ROL2(s[7], 6);
		s[7] = ROL2(s[10], 3);
		s[10] = ROL2(v, 1);
		/* chi: a[i,j] ^= ~b[i,j+1] & b[i,j+2] */
#pragma unroll 5
		for (int j = 0; j<25; j += 5){
			v = s[j]; w = s[j + 1]; s[j] = chi(v, w, s[j + 2]); s[j + 1] = chi(w, s[j + 2], s[j + 3]); s[j + 2] = chi(s[j + 2], s[j + 3], s[j + 4]); s[j + 3] = chi(s[j + 3], s[j + 4], v); s[j + 4] = chi(s[j + 4], v, w);
		}
		/* iota: a[0,0] ^= round constant */
		s[0] ^= keccak_round_constants[0];

#if __CUDA_ARCH__ > 500
#pragma unroll 4
#else
#pragma unroll 3
#endif
		for (int i = 1; i < 23; i++) 
		{
			/*theta*/
#pragma unroll 5
			for (int j = 0; j<5; j++){
				t[j] = vectorize(xor5(devectorize(s[j]), devectorize(s[j + 5]), devectorize(s[j + 10]), devectorize(s[j + 15]), devectorize(s[j + 20])));
			}

			/*theta*/
#pragma unroll 5
			for (int j = 0; j<5; j++){
				u[j] = ROL2(t[j], 1);
			}
			s[4] = xor3x(s[4], t[3], u[0]); s[9] = xor3x(s[9], t[3], u[0]); s[14] = xor3x(s[14], t[3], u[0]); s[19] = xor3x(s[19], t[3], u[0]); s[24] = xor3x(s[24], t[3], u[0]);
			s[0] = xor3x(s[0], t[4], u[1]); s[5] = xor3x(s[5], t[4], u[1]); s[10] = xor3x(s[10], t[4], u[1]); s[15] = xor3x(s[15], t[4], u[1]); s[20] = xor3x(s[20], t[4], u[1]);
			s[1] = xor3x(s[1], t[0], u[2]); s[6] = xor3x(s[6], t[0], u[2]); s[11] = xor3x(s[11], t[0], u[2]); s[16] = xor3x(s[16], t[0], u[2]); s[21] = xor3x(s[21], t[0], u[2]);
			s[2] = xor3x(s[2], t[1], u[3]); s[7] = xor3x(s[7], t[1], u[3]); s[12] = xor3x(s[12], t[1], u[3]); s[17] = xor3x(s[17], t[1], u[3]); s[22] = xor3x(s[22], t[1], u[3]);
			s[3] = xor3x(s[3], t[2], u[4]); s[8] = xor3x(s[8], t[2], u[4]); s[13] = xor3x(s[13], t[2], u[4]); s[18] = xor3x(s[18], t[2], u[4]); s[23] = xor3x(s[23], t[2], u[4]);

			/* rho pi: b[..] = rotl(a[..], ..) */
			v = s[1];
			s[1] = ROL2(s[6], 44);
			s[6] = ROL2(s[9], 20);
			s[9] = ROL2(s[22], 61);
			s[22] = ROL2(s[14], 39);
			s[14] = ROL2(s[20], 18);
			s[20] = ROL2(s[2], 62);
			s[2] = ROL2(s[12], 43);
			s[12] = ROL2(s[13], 25);
			s[13] = ROL8(s[19]);
			s[19] = ROR8(s[23]);
			s[23] = ROL2(s[15], 41);
			s[15] = ROL2(s[4], 27);
			s[4] = ROL2(s[24], 14);
			s[24] = ROL2(s[21], 2);
			s[21] = ROL2(s[8], 55);
			s[8] = ROL2(s[16], 45);
			s[16] = ROL2(s[5], 36);
			s[5] = ROL2(s[3], 28);
			s[3] = ROL2(s[18], 21);
			s[18] = ROL2(s[17], 15);
			s[17] = ROL2(s[11], 10);
			s[11] = ROL2(s[7], 6);
			s[7] = ROL2(s[10], 3);
			s[10] = ROL2(v, 1);

			/* chi: a[i,j] ^= ~b[i,j+1] & b[i,j+2] */
#pragma unroll 5
			for (int j = 0; j<25; j += 5){
				v = s[j]; w = s[j + 1]; s[j] = chi(v, w, s[j + 2]); s[j + 1] = chi(w, s[j + 2], s[j + 3]); s[j + 2] = chi(s[j + 2], s[j + 3], s[j + 4]); s[j + 3] = chi(s[j + 3], s[j + 4], v); s[j + 4] = chi(s[j + 4], v, w);
			}

			/* iota: a[0,0] ^= round constant */
			s[0] ^= keccak_round_constants[i];
		}
		/*theta*/
#pragma unroll 5
		for (int j = 0; j<5; j++){
			t[j] = xor3x(xor3x(s[j + 0], s[j + 5], s[j + 10]), s[j + 15], s[j + 20]);
		}
		/*theta*/
#pragma unroll 5
		for (int j = 0; j<5; j++){
			u[j] = ROL2(t[j], 1);
		}
		s[9] = xor3x(s[9], t[3], u[0]);
		s[24] = xor3x(s[24], t[3], u[0]);
		s[0] = xor3x(s[0], t[4], u[1]);
		s[10] = xor3x(s[10], t[4], u[1]);
		s[6] = xor3x(s[6], t[0], u[2]);
		s[16] = xor3x(s[16], t[0], u[2]);
		s[12] = xor3x(s[12], t[1], u[3]);
		s[22] = xor3x(s[22], t[1], u[3]);
		s[3] = xor3x(s[3], t[2], u[4]);
		s[18] = xor3x(s[18], t[2], u[4]);
		/* rho pi: b[..] = rotl(a[..], ..) */
		s[1] = ROL2(s[6], 44);
		s[2] = ROL2(s[12], 43);
		s[5] = ROL2(s[3], 28);
		s[7] = ROL2(s[10], 3);
		s[3] = ROL2(s[18], 21);
		s[4] = ROL2(s[24], 14);
		s[6] = ROL2(s[9], 20);
		s[8] = ROL2(s[16], 45);
		s[9] = ROL2(s[22], 61);
		/* chi: a[i,j] ^= ~b[i,j+1] & b[i,j+2] */
		v = s[0]; w = s[1]; s[0] = chi(v, w, s[2]); s[1] = chi(w, s[2], s[3]); s[2] = chi(s[2], s[3], s[4]); s[3] = chi(s[3], s[4], v); s[4] = chi(s[4], v, w);
		v = s[5]; w = s[6]; s[5] = chi(v, w, s[7]); s[6] = chi(w, s[7], s[8]); s[7] = chi(s[7], s[8], s[9]);
		/* iota: a[0,0] ^= round constant */
		s[0] ^= keccak_round_constants[23];

		*(uint2x4*)&inout[0] = *(uint2x4*)&s[0];
		*(uint2x4*)&inout[4] = *(uint2x4*)&s[4];

	}
}


__global__ __launch_bounds__(512, 2)
void tiger192sha512_gpu_hash_64_rtx(int threads, uint32_t *d_hash)
{
	__shared__ uint64_t sharedMem[1024][4];
	if (threadIdx.x < 256)
	{
		sharedMem[threadIdx.x][0] = T1[threadIdx.x];
		sharedMem[threadIdx.x][1] = T1[threadIdx.x];
		sharedMem[threadIdx.x][2] = T1[threadIdx.x];
		sharedMem[threadIdx.x][3] = T1[threadIdx.x];
		sharedMem[threadIdx.x + 256][0] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][1] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][2] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][3] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 512][0] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][1] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][2] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][3] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 768][0] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][1] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][2] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][3] = T4[threadIdx.x];
	}
	__syncthreads();

	int thread = (blockDim.x * blockIdx.x + threadIdx.x);
	if (thread < threads)
	{
		const int index = threadIdx.x & 3;

		uint64_t* inout = (uint64_t*)&d_hash[thread << 4];
		uint64_t buf[3], in[8], in2[8];

#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = inout[i];

#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

		TIGER_ROUND_BODY_RTX(in, buf);

		in2[0] = 1;
#pragma unroll
		for (int i = 1; i < 7; i++) in2[i] = 0;
		in2[7] = 0x200;
		TIGER_ROUND_BODY_RTX(in2, buf);

		const uint32_t thread = (blockDim.x * blockIdx.x + threadIdx.x);
		const uint64_t IV512[8] = {
			0x6A09E667F3BCC908, 0xBB67AE8584CAA73B, 0x3C6EF372FE94F82B, 0xA54FF53A5F1D36F1,
			0x510E527FADE682D1, 0x9B05688C2B3E6C1F, 0x1F83D9ABFB41BD6B, 0x5BE0CD19137E2179
		};
		uint64_t r[8];
		uint64_t W[80];
		//			uint64_t *pHash = &g_hash[thread << 3];
		W[0] = cuda_swab64(buf[0]);
		W[1] = cuda_swab64(buf[1]);
		W[2] = cuda_swab64(buf[2]);
		W[3] = 0;
		W[4] = 0;
		W[5] = 0;
		W[6] = 0;
		W[7] = 0;

		//			*(uint2x4*)&W[0] = *(uint2x4*)&pHash[0];
		//			*(uint2x4*)&W[4] = *(uint2x4*)&pHash[4];

		//	#pragma unroll
		//		for (int i = 0; i < 8; i++) {
		//			W[i] = cuda_swab64(W[i]);
		//		}
		W[8] = 0x8000000000000000;

#pragma unroll
		for (int i = 9; i < 15; i++) {
			W[i] = 0U;
		}
		W[15] = 0x0000000000000200;

#pragma unroll 64
		for (int i = 16; i < 80; i++) {
			W[i] = W[i - 7] + W[i - 16] + SSG5_0(W[i - 15]) + SSG5_1(W[i - 2]);
		}

		*(uint2x4*)&r[0] = *(uint2x4*)&IV512[0];
		*(uint2x4*)&r[4] = *(uint2x4*)&IV512[4];

		uint64_t t1;
#pragma unroll 16
		for (int i = 0; i < 16; i++)
		{
			t1 = W[i] + r[7] + bsg5_1_c(r[4]) + xandx64(r[4], r[5], r[6]) + K_512[i];
#pragma unroll
			for (int l = 6; l >= 0; l--) r[l + 1] = r[l];
			r[0] = t1 + andor64(r[1], r[2], r[3]) + bsg5_0_c(r[1]);
			r[4] += t1;
		}

#pragma unroll
		for (int i = 16; i < 80; i += 16)
		{
#pragma unroll
			for (uint32_t j = 0; j < 16; j++){
				W[j] = ssg5_1(W[(j + 14) & 15]) + ssg5_0(W[(j + 1) & 15]) + W[j] + W[(j + 9) & 15];
			}

#pragma unroll
			for (uint32_t j = 0; j < 16; j++){
				t1 = r[7] + W[j] + bsg5_1(r[4]) + xandx64(r[4], r[5], r[6]) + K_512[i + j];
				//				t1 = r[ 7] + W[j] + K_512[i+j] + xandx64(r[ 4], r[ 5], r[ 6]) + bsg5_1(r[ 4]);
#pragma unroll
				for (int l = 6; l >= 0; l--) r[l + 1] = r[l];
				r[0] = t1 + andor64(r[1], r[2], r[3]) + bsg5_0(r[1]);
				r[4] += t1;
			}
		}

#pragma unroll
		for (int u = 0; u < 8; u++) {
			r[u] = cuda_swab64(r[u] + IV512[u]);
		}
		*(uint2x4*)&inout[0] = *(uint2x4*)&r[0];
		*(uint2x4*)&inout[4] = *(uint2x4*)&r[4];
	}
}


__global__ __launch_bounds__(512, 2)
void tiger192sha512_gpu_hash_64_rtx_final(int threads, uint32_t *d_hash, uint32_t *resNonce, const uint64_t target)
{
	__shared__ uint64_t sharedMem[1024][4];
	if (threadIdx.x < 256)
	{
		sharedMem[threadIdx.x][0] = T1[threadIdx.x];
		sharedMem[threadIdx.x][1] = T1[threadIdx.x];
		sharedMem[threadIdx.x][2] = T1[threadIdx.x];
		sharedMem[threadIdx.x][3] = T1[threadIdx.x];
		sharedMem[threadIdx.x + 256][0] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][1] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][2] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][3] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 512][0] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][1] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][2] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][3] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 768][0] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][1] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][2] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][3] = T4[threadIdx.x];
	}
	__syncthreads();

	int thread = (blockDim.x * blockIdx.x + threadIdx.x);
	if (thread < threads)
	{
		const int index = threadIdx.x & 3;

		uint64_t* inout = (uint64_t*)&d_hash[thread << 4];
		uint64_t buf[3], in[8], in2[8];

#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = inout[i];

#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

		TIGER_ROUND_BODY_RTX(in, buf);

		in2[0] = 1;
#pragma unroll
		for (int i = 1; i < 7; i++) in2[i] = 0;
		in2[7] = 0x200;
		TIGER_ROUND_BODY_RTX(in2, buf);

/*		inout[0] = buf[0];
		inout[1] = buf[1];
		inout[2] = buf[2];
		inout[3] = 0;
		inout[3] = 0;
		inout[4] = 0;
		inout[5] = 0;
		inout[6] = 0;
		inout[7] = 0;
*/
		const uint32_t thread = (blockDim.x * blockIdx.x + threadIdx.x);
		const uint64_t IV512[8] = {
			0x6A09E667F3BCC908, 0xBB67AE8584CAA73B, 0x3C6EF372FE94F82B, 0xA54FF53A5F1D36F1,
			0x510E527FADE682D1, 0x9B05688C2B3E6C1F, 0x1F83D9ABFB41BD6B, 0x5BE0CD19137E2179
		};
		uint64_t r[8];
		uint64_t W[80];
		//			uint64_t *pHash = &g_hash[thread << 3];
		W[0] = cuda_swab64(buf[0]);
		W[1] = cuda_swab64(buf[1]);
		W[2] = cuda_swab64(buf[2]);
		W[3] = 0;
		W[4] = 0;
		W[5] = 0;
		W[6] = 0;
		W[7] = 0;

		//			*(uint2x4*)&W[0] = *(uint2x4*)&pHash[0];
		//			*(uint2x4*)&W[4] = *(uint2x4*)&pHash[4];

		//	#pragma unroll
		//		for (int i = 0; i < 8; i++) {
		//			W[i] = cuda_swab64(W[i]);
		//		}
		W[8] = 0x8000000000000000;

#pragma unroll
		for (int i = 9; i < 15; i++) {
			W[i] = 0U;
		}
		W[15] = 0x0000000000000200;

#pragma unroll 64
		for (int i = 16; i < 80; i++) {
			W[i] = W[i - 7] + W[i - 16] + SSG5_0(W[i - 15]) + SSG5_1(W[i - 2]);
		}

		*(uint2x4*)&r[0] = *(uint2x4*)&IV512[0];
		*(uint2x4*)&r[4] = *(uint2x4*)&IV512[4];

		uint64_t t1;
#pragma unroll 16
		for (int i = 0; i < 16; i++)
		{
			t1 = W[i] + r[7] + bsg5_1_c(r[4]) + xandx64(r[4], r[5], r[6]) + K_512[i];
#pragma unroll
			for (int l = 6; l >= 0; l--) r[l + 1] = r[l];
			r[0] = t1 + andor64(r[1], r[2], r[3]) + bsg5_0_c(r[1]);
			r[4] += t1;
		}

#pragma unroll
		for (int i = 16; i < 80; i += 16)
		{
#pragma unroll
			for (uint32_t j = 0; j < 16; j++){
				W[j] = ssg5_1(W[(j + 14) & 15]) + ssg5_0(W[(j + 1) & 15]) + W[j] + W[(j + 9) & 15];
			}

#pragma unroll
			for (uint32_t j = 0; j < 16; j++){
				t1 = r[7] + W[j] + bsg5_1(r[4]) + xandx64(r[4], r[5], r[6]) + K_512[i + j];
				//				t1 = r[ 7] + W[j] + K_512[i+j] + xandx64(r[ 4], r[ 5], r[ 6]) + bsg5_1(r[ 4]);
#pragma unroll
				for (int l = 6; l >= 0; l--) r[l + 1] = r[l];
				r[0] = t1 + andor64(r[1], r[2], r[3]) + bsg5_0(r[1]);
				r[4] += t1;
			}
		}

//#pragma unroll
//		for (int u = 0; u < 8; u++) {
//			r[u] = cuda_swab64(r[u] + IV512[u]);
//		}
		r[3] = cuda_swab64(r[3] + IV512[3]);

		if (r[3] <= target)
		{
			uint32_t tmp = atomicExch(&resNonce[0], thread);
			if (tmp != UINT32_MAX)
				resNonce[1] = tmp;
		}

//		*(uint2x4*)&inout[0] = *(uint2x4*)&r[0];
//		*(uint2x4*)&inout[4] = *(uint2x4*)&r[4];

	}
}

__global__ __launch_bounds__(512, 2)
void tiger192sha512_gpu_hash_64(int threads, uint32_t *d_hash)
{
	__shared__ uint64_t sharedMem[768];
	if (threadIdx.x < 256)
	{
		sharedMem[threadIdx.x] = T1[threadIdx.x];
		sharedMem[threadIdx.x + 256] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 512] = T3[threadIdx.x];
//		sharedMem[threadIdx.x + 768] = T4[threadIdx.x];
	}
	__syncthreads();

	int thread = (blockDim.x * blockIdx.x + threadIdx.x);
	if (thread < threads)
	{
		uint64_t* inout = (uint64_t*)&d_hash[thread << 4];
		uint64_t buf[3], in[8], in2[8];

#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = inout[i];

#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

		TIGER_ROUND_BODY(in, buf);

		in2[0] = 1;
#pragma unroll
		for (int i = 1; i < 7; i++) in2[i] = 0;
		in2[7] = 0x200;
		TIGER_ROUND_BODY(in2, buf);

		inout[0] = buf[0];
		inout[1] = buf[1];
		inout[2] = buf[2];
		inout[3] = 0;
		inout[4] = 0;
		inout[5] = 0;
		inout[6] = 0;
		inout[7] = 0;


		const uint32_t thread = (blockDim.x * blockIdx.x + threadIdx.x);
		const uint64_t IV512[8] = {
			0x6A09E667F3BCC908, 0xBB67AE8584CAA73B, 0x3C6EF372FE94F82B, 0xA54FF53A5F1D36F1,
			0x510E527FADE682D1, 0x9B05688C2B3E6C1F, 0x1F83D9ABFB41BD6B, 0x5BE0CD19137E2179
		};
		uint64_t r[8];
		uint64_t W[80];
		//			uint64_t *pHash = &g_hash[thread << 3];
		W[0] = cuda_swab64(inout[0]);
		W[1] = cuda_swab64(inout[1]);
		W[2] = cuda_swab64(inout[2]);
		W[3] = 0;
		W[4] = 0;
		W[5] = 0;
		W[6] = 0;
		W[7] = 0;

		//			*(uint2x4*)&W[0] = *(uint2x4*)&pHash[0];
		//			*(uint2x4*)&W[4] = *(uint2x4*)&pHash[4];

		//	#pragma unroll
		//		for (int i = 0; i < 8; i++) {
		//			W[i] = cuda_swab64(W[i]);
		//		}
		W[8] = 0x8000000000000000;

#pragma unroll
		for (int i = 9; i < 15; i++) {
			W[i] = 0U;
		}
		W[15] = 0x0000000000000200;

#pragma unroll 64
		for (int i = 16; i < 80; i++) {
			W[i] = W[i - 7] + W[i - 16] + SSG5_0(W[i - 15]) + SSG5_1(W[i - 2]);
		}

		*(uint2x4*)&r[0] = *(uint2x4*)&IV512[0];
		*(uint2x4*)&r[4] = *(uint2x4*)&IV512[4];

		uint64_t t1;
#pragma unroll 16
		for (int i = 0; i < 16; i++)
		{
			t1 = W[i] + r[7] + bsg5_1_c(r[4]) + xandx64(r[4], r[5], r[6]) + K_512[i];
#pragma unroll
			for (int l = 6; l >= 0; l--) r[l + 1] = r[l];
			r[0] = t1 + andor64(r[1], r[2], r[3]) + bsg5_0_c(r[1]);
			r[4] += t1;
		}

#pragma unroll
		for (int i = 16; i < 80; i += 16)
		{
#pragma unroll
			for (uint32_t j = 0; j < 16; j++){
				W[j] = ssg5_1(W[(j + 14) & 15]) + ssg5_0(W[(j + 1) & 15]) + W[j] + W[(j + 9) & 15];
			}

#pragma unroll
			for (uint32_t j = 0; j < 16; j++){
				t1 = r[7] + W[j] + bsg5_1(r[4]) + xandx64(r[4], r[5], r[6]) + K_512[i + j];
				//				t1 = r[ 7] + W[j] + K_512[i+j] + xandx64(r[ 4], r[ 5], r[ 6]) + bsg5_1(r[ 4]);
#pragma unroll
				for (int l = 6; l >= 0; l--) r[l + 1] = r[l];
				r[0] = t1 + andor64(r[1], r[2], r[3]) + bsg5_0(r[1]);
				r[4] += t1;
			}
		}

#pragma unroll
		for (int u = 0; u < 8; u++) {
			r[u] = cuda_swab64(r[u] + IV512[u]);
		}

		*(uint2x4*)&inout[0] = *(uint2x4*)&r[0];
		*(uint2x4*)&inout[4] = *(uint2x4*)&r[4];

	}
}

__global__ void __launch_bounds__(256,5) tiger192_gpu_hash_80(int threads, uint32_t startNonce, uint32_t *d_hash)
{
	__shared__ uint64_t sharedMem[1024];
//	if(threadIdx.x < 256)
	{
		sharedMem[threadIdx.x]      = T1[threadIdx.x];
		sharedMem[threadIdx.x+256]  = T2[threadIdx.x];
		sharedMem[threadIdx.x+512]  = T3[threadIdx.x];
		sharedMem[threadIdx.x+768]  = T4[threadIdx.x];
	}
	__syncthreads();

  int thread = (blockDim.x * blockIdx.x + threadIdx.x);
  if (thread < threads) {
		uint64_t* out = (uint64_t*)&d_hash[thread<<4];
		uint64_t buf[3], in[8], in2[8];

        const uint32_t nonce = cuda_swab32(startNonce + thread);

		#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = c_PaddedMessage80[i];

		#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

        TIGER_ROUND_BODY(in, buf);

		in2[0] = c_PaddedMessage80[8];
		in2[1] = (((uint64_t) nonce) << 32) | (c_PaddedMessage80[9] & 0xffffffff);
        in2[2] = 1;
        #pragma unroll
        for (int i = 3; i < 7; i++) in2[i] = 0;
		in2[7] = 0x280;

		TIGER_ROUND_BODY(in2, buf);

		#pragma unroll
		for (int i = 0; i < 3; i++) out[i] = buf[i];
		#pragma unroll
		for (int i = 3; i < 8; i++) out[i] = 0;
  }
}

__global__  __launch_bounds__(512, 2)
void tiger192sha512_gpu_hash_80_rtx(int threads, uint32_t startNonce, uint32_t *d_hash)
{
	__shared__ uint64_t sharedMem[1024][4];
	if (threadIdx.x < 256)
	{
		sharedMem[threadIdx.x][0] = T1[threadIdx.x];
		sharedMem[threadIdx.x][1] = T1[threadIdx.x];
		sharedMem[threadIdx.x][2] = T1[threadIdx.x];
		sharedMem[threadIdx.x][3] = T1[threadIdx.x];
		sharedMem[threadIdx.x + 256][0] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][1] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][2] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][3] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 512][0] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][1] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][2] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][3] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 768][0] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][1] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][2] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][3] = T4[threadIdx.x];
	}
	__syncthreads();

	int thread = (blockDim.x * blockIdx.x + threadIdx.x);
	if (thread < threads)
	{
		const int index = threadIdx.x & 3;
		uint64_t* out = (uint64_t*)&d_hash[thread << 4];
		uint64_t buf[3], in[8], in2[8];

		const uint32_t nonce = cuda_swab32(startNonce + thread);

#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = c_PaddedMessage80[i];

#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

		TIGER_ROUND_BODY_RTX(in, buf);

		in2[0] = c_PaddedMessage80[8];
		in2[1] = (((uint64_t)nonce) << 32) | (c_PaddedMessage80[9] & 0xffffffff);
		in2[2] = 1;
#pragma unroll
		for (int i = 3; i < 7; i++) in2[i] = 0;
		in2[7] = 0x280;

		TIGER_ROUND_BODY_RTX(in2, buf);

		const uint32_t thread = (blockDim.x * blockIdx.x + threadIdx.x);
		const uint64_t IV512[8] = {
			0x6A09E667F3BCC908, 0xBB67AE8584CAA73B, 0x3C6EF372FE94F82B, 0xA54FF53A5F1D36F1,
			0x510E527FADE682D1, 0x9B05688C2B3E6C1F, 0x1F83D9ABFB41BD6B, 0x5BE0CD19137E2179
		};
		uint64_t r[8];
		uint64_t W[80];
		//			uint64_t *pHash = &g_hash[thread << 3];
		W[0] = cuda_swab64(buf[0]);
		W[1] = cuda_swab64(buf[1]);
		W[2] = cuda_swab64(buf[2]);
		W[3] = 0;
		W[4] = 0;
		W[5] = 0;
		W[6] = 0;
		W[7] = 0;
		W[8] = 0x8000000000000000;

#pragma unroll
		for (int i = 9; i < 15; i++) {
			W[i] = 0U;
		}
		W[15] = 0x0000000000000200;

#pragma unroll 64
		for (int i = 16; i < 80; i++) {
			W[i] = W[i - 7] + W[i - 16] + SSG5_0(W[i - 15]) + SSG5_1(W[i - 2]);
		}

		*(uint2x4*)&r[0] = *(uint2x4*)&IV512[0];
		*(uint2x4*)&r[4] = *(uint2x4*)&IV512[4];

		uint64_t t1;
#pragma unroll 16
		for (int i = 0; i < 16; i++)
		{
			t1 = W[i] + r[7] + bsg5_1_c(r[4]) + xandx64(r[4], r[5], r[6]) + K_512[i];
#pragma unroll
			for (int l = 6; l >= 0; l--) r[l + 1] = r[l];
			r[0] = t1 + andor64(r[1], r[2], r[3]) + bsg5_0_c(r[1]);
			r[4] += t1;
		}

#pragma unroll
		for (int i = 16; i < 80; i += 16)
		{
#pragma unroll
			for (uint32_t j = 0; j < 16; j++){
				W[j] = ssg5_1(W[(j + 14) & 15]) + ssg5_0(W[(j + 1) & 15]) + W[j] + W[(j + 9) & 15];
			}

#pragma unroll
			for (uint32_t j = 0; j < 16; j++){
				t1 = r[7] + W[j] + bsg5_1(r[4]) + xandx64(r[4], r[5], r[6]) + K_512[i + j];
				//				t1 = r[ 7] + W[j] + K_512[i+j] + xandx64(r[ 4], r[ 5], r[ 6]) + bsg5_1(r[ 4]);
#pragma unroll
				for (int l = 6; l >= 0; l--) r[l + 1] = r[l];
				r[0] = t1 + andor64(r[1], r[2], r[3]) + bsg5_0(r[1]);
				r[4] += t1;
			}
		}

#pragma unroll
		for (int u = 0; u < 8; u++) {
			r[u] = cuda_swab64(r[u] + IV512[u]);
		}
		*(uint2x4*)&out[0] = *(uint2x4*)&r[0];
		*(uint2x4*)&out[4] = *(uint2x4*)&r[4];

	}
}

__global__  __launch_bounds__(512, 2)
void tiger192luffa512_gpu_hash_80_rtx(int threads, uint32_t startNonce, uint32_t *d_hash)
{
	__shared__ uint64_t sharedMem[1024][4];
	if (threadIdx.x < 256)
	{
		sharedMem[threadIdx.x][0] = T1[threadIdx.x];
		sharedMem[threadIdx.x][1] = T1[threadIdx.x];
		sharedMem[threadIdx.x][2] = T1[threadIdx.x];
		sharedMem[threadIdx.x][3] = T1[threadIdx.x];
		sharedMem[threadIdx.x + 256][0] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][1] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][2] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][3] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 512][0] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][1] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][2] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][3] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 768][0] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][1] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][2] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][3] = T4[threadIdx.x];
	}
	__syncthreads();

	int thread = (blockDim.x * blockIdx.x + threadIdx.x);
	if (thread < threads)
	{
		const int index = threadIdx.x & 3;
		uint64_t* out = (uint64_t*)&d_hash[thread << 4];
		uint64_t buf[3], in[8], in2[8];

		const uint32_t nonce = cuda_swab32(startNonce + thread);

#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = c_PaddedMessage80[i];

#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

		TIGER_ROUND_BODY_RTX(in, buf);

		in2[0] = c_PaddedMessage80[8];
		in2[1] = (((uint64_t)nonce) << 32) | (c_PaddedMessage80[9] & 0xffffffff);
		in2[2] = 1;
#pragma unroll
		for (int i = 3; i < 7; i++) in2[i] = 0;
		in2[7] = 0x280;

		TIGER_ROUND_BODY_RTX(in2, buf);

		uint32_t statebuffer[8];

		uint32_t statechainv[40] = {
			0x8bb0a761, 0xc2e4aa8b, 0x2d539bc9, 0x381408f8, 0x478f6633, 0x255a46ff, 0x581c37f7, 0x601c2e8e,
			0x266c5f9d, 0xc34715d8, 0x8900670e, 0x51a540be, 0xe4ce69fb, 0x5089f4d4, 0x3cc0a506, 0x609bcb02,
			0xa4e3cd82, 0xd24fd6ca, 0xc0f196dc, 0xcf41eafe, 0x0ff2e673, 0x303804f2, 0xa7b3cd48, 0x677addd4,
			0x66e66a8a, 0x2303208f, 0x486dafb4, 0xc0d37dc6, 0x634d15af, 0xe5af6747, 0x10af7e38, 0xee7e6428,
			0x01262e5d, 0xc92c2e64, 0x82fee966, 0xcea738d3, 0x867de2b0, 0xe0714818, 0xda6e831f, 0xa7062529
		};
		//			uint2x4* Hash = (uint2x4*)&g_hash[thread << 4];

		uint32_t hash[16];
		uint64_t *peker = (uint64_t *)&hash[0];

		peker[0] = buf[0];
		peker[1] = buf[1];
		peker[2] = buf[2];
		peker[3] = 0;
		peker[4] = 0;
		peker[5] = 0;
		peker[6] = 0;
		peker[7] = 0;


		//			*(uint2x4*)&hash[0] = *(uint2x4*)&Hash[0];
		//			*(uint2x4*)&hash[8] = *(uint2x4*)&Hash[4];

#pragma unroll 8
		for (int i = 0; i < 8; i++){
			statebuffer[i] = cuda_swab32(hash[i]);
		}

		rnd512_first(statechainv, statebuffer);

#pragma unroll 8
		for (int i = 0; i < 8; i++){
			statebuffer[i] = cuda_swab32(hash[8 + i]);
		}

		rnd512(statebuffer, statechainv);

		statebuffer[0] = 0x80000000;
#pragma unroll 7
		for (uint32_t i = 1; i < 8; i++)
			statebuffer[i] = 0;

		rnd512(statebuffer, statechainv);

		/*---- blank round with m=0 ----*/
		rnd512_nullhash(statechainv);

#pragma unroll 8
		for (int i = 0; i < 8; i++)
			hash[i] = cuda_swab32(statechainv[i] ^ statechainv[i + 8] ^ statechainv[i + 16] ^ statechainv[i + 24] ^ statechainv[i + 32]);

		rnd512_nullhash(statechainv);

#pragma unroll 8
		for (int i = 0; i < 8; i++)
			hash[8 + i] = cuda_swab32(statechainv[i] ^ statechainv[i + 8] ^ statechainv[i + 16] ^ statechainv[i + 24] ^ statechainv[i + 32]);

		//*(uint2x4*)&inout[0] = *(uint2x4*)&hash[0];
		//*(uint2x4*)&inout[1] = *(uint2x4*)&hash[8];
		out[0] = peker[0];
		out[1] = peker[1];
		out[2] = peker[2];
		out[3] = peker[3];
		out[4] = peker[4];
		out[5] = peker[5];
		out[6] = peker[6];
		out[7] = peker[7];
	}
}


__global__ void __launch_bounds__(256, 5) tiger192_gpu_hash_80_rtx(int threads, uint32_t startNonce, uint32_t *d_hash)
{
	__shared__ uint64_t sharedMem[1024][4];
	if (threadIdx.x < 256)
	{
		sharedMem[threadIdx.x][0] = T1[threadIdx.x];
		sharedMem[threadIdx.x][1] = T1[threadIdx.x];
		sharedMem[threadIdx.x][2] = T1[threadIdx.x];
		sharedMem[threadIdx.x][3] = T1[threadIdx.x];
		sharedMem[threadIdx.x + 256][0] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][1] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][2] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 256][3] = T2[threadIdx.x];
		sharedMem[threadIdx.x + 512][0] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][1] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][2] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 512][3] = T3[threadIdx.x];
		sharedMem[threadIdx.x + 768][0] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][1] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][2] = T4[threadIdx.x];
		sharedMem[threadIdx.x + 768][3] = T4[threadIdx.x];
	}
	__syncthreads();

	int thread = (blockDim.x * blockIdx.x + threadIdx.x);
	if (thread < threads)
	{
		const int index = threadIdx.x & 3;
		uint64_t* out = (uint64_t*)&d_hash[thread << 4];
		uint64_t buf[3], in[8], in2[8];

		const uint32_t nonce = cuda_swab32(startNonce + thread);

#pragma unroll
		for (int i = 0; i < 8; i++) in[i] = c_PaddedMessage80[i];

#pragma unroll
		for (int i = 0; i < 3; i++) buf[i] = III[i];

		TIGER_ROUND_BODY_RTX(in, buf);

		in2[0] = c_PaddedMessage80[8];
		in2[1] = (((uint64_t)nonce) << 32) | (c_PaddedMessage80[9] & 0xffffffff);
		in2[2] = 1;
#pragma unroll
		for (int i = 3; i < 7; i++) in2[i] = 0;
		in2[7] = 0x280;

		TIGER_ROUND_BODY_RTX(in2, buf);

#pragma unroll
		for (int i = 0; i < 3; i++) out[i] = buf[i];
#pragma unroll
		for (int i = 3; i < 8; i++) out[i] = 0;
	}
}


__host__ void tiger192_cpu_hash_64(int thr_id, int threads, int zero_pad_64, uint32_t *d_hash)
{
	const int threadsperblock = 256;
	dim3 grid(threads/threadsperblock);
	dim3 block(threadsperblock);
	tiger192_gpu_hash_64<<<grid, block>>>(threads, d_hash);
}
__host__ void tiger192_cpu_hash_64_rtx(int thr_id, int threads, int zero_pad_64, uint32_t *d_hash)
{
	const int threadsperblock = 256;
	dim3 grid(threads / threadsperblock);
	dim3 block(threadsperblock);
	tiger192_gpu_hash_64_rtx << <grid, block >> >(threads, d_hash);
}

__host__ void tiger192keccak512_cpu_hash_64_rtx(int thr_id, int threads, int zero_pad_64, uint32_t *d_hash)
{
	const int threadsperblock = 256;
	dim3 grid(threads / threadsperblock);
	dim3 block(threadsperblock);
	tiger192Keccak512_gpu_hash_64_rtx << <grid, block >> >(threads, d_hash);
}


__host__ void tiger192sha512_cpu_hash_64_rtx(int thr_id, int threads, int zero_pad_64, uint32_t *d_hash)
{
	const int threadsperblock = 512;
	dim3 grid(threads / threadsperblock);
	dim3 block(threadsperblock);
	tiger192sha512_gpu_hash_64_rtx << <grid, block >> >(threads, d_hash);
}

__host__ void tiger192Luffa512_cpu_hash_64_rtx(int thr_id, int threads, int zero_pad_64, uint32_t *d_hash)
{
	const int threadsperblock = 512;
	dim3 grid(threads / threadsperblock);
	dim3 block(threadsperblock);
	tiger192Luffa512_gpu_hash_64_rtx << <grid, block >> >(threads, d_hash);
}


__host__
void tiger192sha512_cpu_hash_64_rtx_final(int thr_id, uint32_t threads, uint32_t *d_hash, uint32_t *resNonce, const uint64_t target)
{
	const uint32_t threadsperblock = 512;

	dim3 grid((threads + threadsperblock - 1) / threadsperblock);
	dim3 block(threadsperblock);

	tiger192sha512_gpu_hash_64_rtx_final << <grid, block >> > (threads, d_hash, resNonce, target);
}

__host__
void tiger192luffa512_cpu_hash_64_rtx_final(int thr_id, uint32_t threads, uint32_t *d_hash, uint32_t *resNonce, const uint64_t target)
{
	const uint32_t threadsperblock = 512;

	dim3 grid((threads + threadsperblock - 1) / threadsperblock);
	dim3 block(threadsperblock);

	tiger192Luffa512_gpu_hash_64_rtx_final << <grid, block >> > (threads, d_hash, resNonce, target);
}
__host__
void tiger192keccak512_cpu_hash_64_rtx_final(int thr_id, uint32_t threads, uint32_t *d_hash, uint32_t *resNonce, const uint64_t target)
{
	const uint32_t threadsperblock = 512;

	dim3 grid((threads + threadsperblock - 1) / threadsperblock);
	dim3 block(threadsperblock);

	tiger192keccak512_gpu_hash_64_rtx_final << <grid, block >> > (threads, d_hash, resNonce, target);
}


__host__
void tiger192_setBlock_80(void *pdata)
{
    cudaMemcpyToSymbol(c_PaddedMessage80, pdata, sizeof(c_PaddedMessage80), 0, cudaMemcpyHostToDevice);
}

__host__ void tiger192_cpu_hash_80(int thr_id, int threads, uint32_t startNonce, uint32_t *d_hash)
{
	const int threadsperblock = 256;
	dim3 grid(threads/threadsperblock);
	dim3 block(threadsperblock);
	tiger192_gpu_hash_80<<<grid, block>>>(threads, startNonce, d_hash);
}

__host__ void tiger192sha512_cpu_hash_80_rtx(int thr_id, int threads, uint32_t startNonce, uint32_t *d_hash)
{
	const int threadsperblock = 512;
	dim3 grid(threads / threadsperblock);
	dim3 block(threadsperblock);
	tiger192sha512_gpu_hash_80_rtx << <grid, block >> >(threads, startNonce, d_hash);
}

__host__ void tiger192luffa512_cpu_hash_80_rtx(int thr_id, int threads, uint32_t startNonce, uint32_t *d_hash)
{
	const int threadsperblock = 512;
	dim3 grid(threads / threadsperblock);
	dim3 block(threadsperblock);
	tiger192luffa512_gpu_hash_80_rtx << <grid, block >> >(threads, startNonce, d_hash);
}

__host__ void tiger192_cpu_hash_80_rtx(int thr_id, int threads, uint32_t startNonce, uint32_t *d_hash)
{
	const int threadsperblock = 256;
	dim3 grid(threads / threadsperblock);
	dim3 block(threadsperblock);
	tiger192_gpu_hash_80_rtx << <grid, block >> >(threads, startNonce, d_hash);
}


